# Estad铆stica Avanzada Aplicada

Elena Igualada Villodre
    - Ingeniera Industrial 
    - Doctora en Matem谩ticas


4 actividades diferentes (10%)
    - 1. Cuestionario tipo test. 
    - 3. Pr谩cticas en Python. 


5 semanas de asignatura
Examen en marzo 


# Contenido del Curso

- Objetivos
    - Objetivos espec铆ficos
    - Resultados de aprendizaje
- Unidad 1. Variables y Estimadores
    - Teor铆a de probabilidad y estad铆stica b谩sica
    - T茅cnicas de descripci贸n y modelado de datos complejos: muestreo, training, test set
    - Introducci贸n al aprendizaje estad铆stico: inferencia, aprendizaje bayesiano, funci贸n de p茅rdida
- Unidad 2. Modelos de Regresi贸n y regularizaci贸n
    - Regresi贸n lineal, no lineal y log铆stica
    - Regresi贸n con m煤ltiples variables
    - Regularizaci贸n: ridge y lasso
    - Regresi贸n no param茅trica: spline y kernel.
Unidad 3. An谩lisis y modelado avanzados
    - rboles de decisi贸n. Modelos gr谩ficos probabilistas.
    - Modelado de funciones de densidad de probabilidad
    - An谩lisis de Series temporales
    - Optimizaci贸n para grandes vol煤menes de datos

# Objetivos

## Objetivos espec铆ficos
- Dise帽ar y aplicar algoritmos de an谩lisis basados en sistemas e infraestructuras de almacenamiento y acceso a grandes vol煤menes de datos. 

## Resultados de aprendizaje

- Comparaci贸n y contraste cr铆ticos de m煤ltiples modelos de sistemas distribuidos y sus tecnolog铆as habilitantes asociadas.

# Unidad 1. Variables y Estimadores

## Probabilidad y variables estimatorias

- Espacio muestral: Al conjunto de todos los escenarios posibles de un experimento .
- Suceso elemental: Cada uno de los posibles resultados de un experimento contenido en un espacio muestral. 
- Suceso compuesto: subconjunto del espacio muestral o una agrupaci贸n de espacios muestrales. 
- Sucesos disjuntos o mutuamente excluyentes: si pasa uno el otro no puede pasar. 
- Enfoque frecuentista de la probabilidad: cree que la probabilidad de que los sucesos elementales sucedan es igual. Cree que en la tendencia a un resultado si lo repetimos una y otra vez. 
- Enfoque axiom谩tico: cree que los sucesos elementales no son igualmente probables de ocurrir. Considera en que la suma de todas las probabilidades de los sucesos elementales debe ser igual a 1. 
- Variable aleatoria: mide una propiedad de la 
- Variables aleatorias discretas --> funci贸n de masa 
- Variables aleatorias continuas --> funci贸n de densidad  
- Modelos probabil铆sticos t铆picos: 
	- Binomial: Dos sucesos elementales: 茅xito/fracaso.
	- Poisson: modelar las probabilidades un tiempo determinado 
	- Exponencial 
	- Uniforme
	-  Modelo normal: sigue el teorema central del l铆mite. 

# 2. Modelos de Regresi贸n y regularizaci贸n 

## Teor铆a de muestreo y partici贸n de datos

- Metodolog铆a para el an谩lisis de datos: 
	- Preparaci贸n de los datos
		- Recogida
		- Limpieza de los datos (eliminar datos duplicados, errores, campos vac铆os y nulos).
		- Transformaci贸n de variables categ贸ricas en num茅ricas. 
	- An谩lisis estad铆stico
		- Estad铆stica descriptiva: Aplicamos un an谩lisis exploratorio de datos (EDA), sin preguntarnos por la poblaci贸n ni hacer inferenc. 
		- An谩lisis estad铆stico: se buscan modelos que se ajusten a nuestros datos
		- Aprendizaje sobre el fen贸meno
		- Creamos un modelo que sea capaz de predecir el comportamiento de una variable futura. 
Teor铆a de muestreo 
	- Muestra vs. Poblaci贸n 
		- Poblaci贸n -objetivo: es un subconjunto del universo que engloba a toda la poblaci贸n que cumple las condiciones para realizar un experimento. 
		- Muestra: una parte representativa de la poblaci贸n. 
	- Distribuci贸n muestral 
		- Necesitamos conocer el nivel de incertidumbre de nuestros estad铆sticos y para ello debemos conocer la distribuci贸n muestral
	- Par谩metros vs Estad铆stico
		- Par谩metro: es una medida descriptiva de una poblaci贸n
		- Estad铆stico: es una medida descriptiva de la muestra. 
- Inferencia estad铆stica
	- Objetivos 
		- Obtener modelo poblacional mediante sus par谩metros. 
		- Medir la exactitud del modelo obtenido y sus estimadores. 
	- M茅todos de inferencia
		- Cl谩sicos
		- Computacionales
			- Monte-Carlo
			- BootStrappping
			- Estimadores bayesianos 
Estimaci贸n param茅trica 
	- Un estimador:  Es un estad铆stico que se deben acercar al par谩metro. 
	- Formas de evaluar un estimador: 
		- Sesgo
		- Error cuadr谩tico medio
		- Eficiencia relativa 
		- Error est谩ndar 
- Diferencias entre estad铆stica cl谩sica vs. Computacional: 
	- Tama帽o de las muestras
	- Distribuci贸n de los datos (homog茅neos vs. No homog茅neos) 
	- Manualmente tratable vs. Computacionalmente tratable
	- Algoritmos sencillos vs. Algoritmos complicados
- Modelizaci贸n: 
	- El objetivo de un modelo es explicar los datos y que sea capaz de predecir, y podamos sacar conclusiones y tomar decisiones. 
	- Debemos encontrar los par谩metros que mejor representen a la poblaci贸n.
	- Hiperpar谩metros: par谩metros propios del modelo. 
- Partici贸n de datos 
	- Training sets: Entrenar el modelo (70%)
	- Validation sets: Si el modelo tiene hiperpar谩metros, evaluar铆amos y seleccionar铆amos los valores 贸ptimos de los hiperpar谩metros. (15%)
	- Test sets: Se usan para probar el rendimiento de nuestro modelo con datos que no se usaron para el entrenamiento (10-20%)
- Validaci贸n cruzada (k-fold validation): 
	- Se usa para determinar el error predictivo del modelo. 
	- Consiste en dividir los datos disponibles en k particiones. En la primera iteraci贸n se utilizan k-1 particiones para entrenar el modelo y la k-es铆ma partici贸n se usa para probar el modelo. En la segunda iteraci贸n utilizamosuna k-sima partici贸n diferente a la primera iteraci贸n y se repite el proceso. Iteramos k-veces, es decir, hasta que todas las particiones hayan pasado como test sets. 
	- Este es el modelo que escogemos para seleccionar los hiperpar谩metros del modelo. 

## Estad铆stica Computacional e Introducci贸n al Aprendizaje Autom谩tico 

Los problemas de ML y aprendizaje autom谩tico se dividen en dos: 
- Supervisados: para cada valor de una variable entrada tenemos otra de salida. 
	- Predicci贸n 
	- Clasificaci贸n 
- No supervisados: no tenemos una variable respuesta asociada
- Metodos de Monte Carlo para la inferencia: siguen una aproximaci贸n param茅trica, es decir, que los datos analizados siguen una distribuci贸n con par谩metros conocidos 
	- Se basan en la generaci贸n de un gran n煤mero de muestras aleatorias, por lo que requieren de gran potencia computacional. 
- M茅todos bootstrapping: Siguen una aproximaci贸n no param茅trica. No se asume que los datos siguen una distribuci贸n conocida. 
- Aprendizaje bayesiano: Se usa el teorema de Bayes para actualizar la probabilidad de una hip贸tesis a medida que tenemos m谩s informaci贸n o evidencias . 
- Modelan la distribuci贸n de incertidumbre de los valores de los par谩metros desconocidos como si fuera una probabilidad. 
	- Es excelente para: 
		- Reconocimiento de patrones
- Funciones de p茅rdidas
	- Tambi茅n llamada funci贸n de costes, es una forma de cuantificar lo bien que funciona un modelo. 
    - A veces se usa el t茅rmino funci贸n de p茅rdida para referirse a un solo experim ent o, mientras que se deja el t茅rmino funci贸n de coste para referirse al conjunto de datos. Esto es, la funci贸n de coste ser铆a el sumatorio de las funciones de p茅rdidas de cada uno de los datos.
	- Si el modelo se ajusta bien a los datos, el valor de la funci贸n de coste ser谩 bajo. 
	- La funci贸n de p茅rdidas es la funci贸n objetivo a minimizar por el algoritmo optimizador  en el algoritmo de ML. 
		- Funciones de p茅rdida para regresi贸n 
			- Error cuadr谩tico medio  (MSE): es la media artim茅tica de la diferencia al cuadrado del valor real de la variable respuesta y el valor predicho. Intensifica la importancia de los valores at铆picos. 
				- Error Cuadr谩tico Medio Modificado: MSE/2
			- Error medio absoluto (MAE): Es parecido al anterior pero en lugar de la diferencia al cuadrado, es la diferencia absoluta. Es dif铆cil calcular su derivada. 
			- Error medio absoluto suavizado (Huber loss): funci贸n a trozos, introduce un nuevo hiperpar谩metro. La elecci贸n de este par谩metro es cr铆tico porque determinar谩 qu茅 valores son outliers. 
		- Funciones de p茅rdida para clasificaci贸n 
			- Cross-entropy: 
				- Funci贸n usada en regresi贸n log铆stica
				- Se usa para problemas de clasificaci贸n binarias. 
		- Hinge Loss
			-  Se usa papra algoritmos llamados Support Vector Machine
			-  Se puede usar para problemas de clasificaci贸n categ贸ricas

## Regresi贸n lineal univariable
-  Minimizar la funci贸n de coste
	- Para encontrar el m铆nimo de una funci贸n, hay que hallar la derivada de esta e igualarla a cero. Observando la f贸rmula de la funci贸n de coste, vemos que tiene forma de paraboloide.
	- Existen diversos m茅todos para minimizar la funci贸n de coste; el m谩s com煤n para ML es el del descenso del gradiente
- Descenso del Gradiente
	- Empezar con un valor inicial de ( 0, 1
	- Calcular el valor de las derivadas parciales de J para dichos valores y actualizar los valores ( 0, 1 ).
	- Repetir hasta que el valor de J no var铆e y hayamos encontrado su
		m铆nimo.
	- para los problemas de regresi贸n lineal, la funci贸n de coste siempre tendr谩 forma convexa, por tanto, el algoritmo del descenso del gradiente siempre va a converger al m铆nimo absoluto.

## Regresi贸n multivariable y no lineal

- Un modelo multivariable es aquel en el que la variable respuesta depende de varias variable independientes o de entrada, tambi茅n llamadas variables explicativas.
- Las variables representan distintas magnitudes, por lo que su rango de
valores puede ser muy distinto.
- colinealidad 
    - Una de las condiciones para que el modelo de regresi贸n m煤ltiple funcione correctamente es que las variables de entrada sean independientes entre s铆. Cuando existe una gran correlaci贸n entre dos variables,  este hecho puede dar lugar a problemas de colinealidad.
- Regresi贸n no lineal: A la hora de formular el problema
no hay diferencia entre el problema lineal multivariable y el problema
no lineal, ya que aplicaremos una funci贸n a las variables no lineales para transformarlas en variables lineales. 
- Coeficiente de determinaci贸n 
    - Es un estad铆stico que nos da una medida de la bondad del modelo ajustado por regresi贸n. Se define como el ratio entre la variabilidad explicada por el modelo y la variabilidad total, y se denota por R2.
    - Se pueden comparar distintos modelos entre s铆 mediante el coeficiente de determinaci贸n de cada modelo. 
    - Cuanto mayor sea el valor de R2 mayor es la capacidad explicativa del modelo. Por tanto.

## Regresi贸n log铆stica
- 
- Se pueden construir modelos de clasificaci贸n binaria usando el modelo de regresi贸n log铆stica.
- El problema de clasificaci贸n puede verse as铆 como un problema de predicci贸n en el que hay que predecir la etiqueta de la variable de salida.
- Superficie de decisi贸n: es una divisi贸n del espacio, en donde cada regi贸n del espacio pertenece a una clase. 
- M茅todos de optimizaci贸n 
    - M谩xima verosimilitud 
    - Descenso de gradiente
    - Optimizaci贸n avanzada

## Regularizaci贸n y selecci贸n del modelo

- El error de predicci贸n puede descomponerse como la suma de la varianza del modelo poblacional, m谩s el sesgo al cuadrado del modelo estimado m谩s la varianza del modelo estimado. 
- Para que el error de predicci贸n sea peque帽o, debemos conseguir que tanto el
sesgo como la varianza del modelo sean peque帽os.
- El sesgo es una medida de la diferencia entre el valor esperado de un par谩metro y su valor real. Un modelo con un sesgo elevado predecir谩 valores muy alejados de los reales.
- La varianza nos da idea de la dispersi贸n de nuestros datos con respecto a la media. 
- Equilibrio entre sesgo y varianza
    - Cuando un modelo produce un ajuste insuficiente de los datos, se dice que tiene un sesgo elevado y una varianza baja.
    - un modelo que est谩 sobreajustado tiene una varianza muy alta y un sesgo normalmente bajo.
- Regularizaci贸n: El m茅todo de regularizaci贸n consiste en construir un modelo lineal complejo, que a priori tenga en cuenta todas las variables de entrada pero
penalizando los valores altos de los coeficientes, forzando a que varios de ellos tengan valores pr贸ximos a cero o incluso iguales a cero. 
    - Esta t茅cnica provoca un peque帽o aumento en el sesgo pero, a cambio, consigue una notable reducci贸n en la varianza. 
    - Hay varios tipos de regularizaci贸n. Algunas de las m谩s usadas son la regularizaci贸n Ridge, la Lasso y elastic net. 
    - Regularizaci贸n Ridge: Es una de las formas de regularizaci贸n m谩s sencillas. Consiste en a帽adir un t茅rmino de penalizaci贸n a la funci贸n de coste, que es igual al cuadrado de la suma de los coeficientes.
    - Regularizaci贸n Lasso: Este m茅todo de regularizaci贸n es muy parecido al anterior, con la salvedad de que el t茅rmino de penalizaci贸n que se a帽ade a la funci贸n de coste es la suma de los valores absolutos de los coeficientes del modelo.

# An谩lisis y modelos avanzados

## Estimaci贸n de funciones de densidad de probabilidad

- Se puede distinguir entre dos grandes grupos de estimaci贸n de funciones de densidad de probabilidad:
    - 1.Estimaci贸n param茅trica:  la funci贸n de densidad es estimada asumiendo un modelo de distribuci贸n conocido (por ej emplo, gaussiano, binomial, etc.), y utilizando los datos disponibles para ajustar los par谩metros del modelo elegido. El problema ser谩 determinar dichos par谩metros.
    - 2. Estimaci贸n no param茅trica o semiparam茅trica: hace uso de t茅cnicas que no hacen suposiciones (o hacen muy pocas y d茅biles) sobre la forma que tiene la funci贸n de densidad de probabilidad.
- m茅todos para calcular el error cometido:
    - el error cuadr谩tico medio integrado o MISE (del ingl茅s mean integrated squared error)
    -  el error medio absoluto integrado o MIAE (del ingl茅s, mean integrated absolute error).
- ancho de bin h determina c贸mo de suave es la forma del histograma, por lo que recibe el nombre de par谩metro de suavizado.
- Estimaci贸n por histogramas
    - Pol铆gonos de frecuencias: son un m茅todo para estimar funciones de densidad de probabilidad, que se basan tambi茅n en histogramas. La funci贸n de densidad de probabilidad se determina por interpolaci贸n lineal entre los puntos medios de cada una de las barras (o bins) del histograma. Es decir, primero se obtendr谩 el histograma, y luego se calcular谩 el valor de la funci贸n de distribuci贸n en base a esta expresi贸n. 
    - M茅todo ASH: El m茅todo ASH (del ingl茅s average shifted histogram) surge para tener en cuenta el origen t0. La idea detr谩s de este m茅todo es crear varios histogramas con igual ancho h y distinto origen t0 y promediarlos. De esta forma se consigue un estimador de la funci贸n de densidad m谩s suave.
- Estimaci贸n Kernel: Normalmente se elige como funci贸n kernel una distribuci贸n de probabilidad conocida, siendo muy habitual elegir la normal est谩ndar. Otro par谩metro es el ancho de bin, h. 


## An谩lisis de series temporales 

- Una serie temporal es una sucesi贸n de observaciones de una variable, o un conjunto de datos, que han sido recogidas de forma secuencial en distintos instantes de tiempo.
- El objetivo del an谩lisis de series temporales es estudiar el comportamiento de la variable con respecto al tiempo para encontrar posibles patrones y para ser capaces de predecir valores futuros de dicha variable.
- El objetivo principal del an谩lisis de una serie temporal es la predicci贸n de valores futuros, aunque tambi茅n interesa analizar la serie en busca de patrones, o para analizar las razones de posibles cambios en el tiempo.
- Representaci贸n de series temporales
    - Gr谩ficas temporales, barras, velas
- Clasificiaci贸n de series temporales: 
    - Si la serie temporal es tal que se puede determinar exactamente un valor futuro, se dice que la serie es determinista. Si, por el contrario, el valor futuro **no se puede predecir con certeza, se dice que la serie es estoc谩stica**.
    - Series temporales estacionarias: su media y su variabilidad se mantienen
constantes a lo largo del tiempo.
        - Siempre es deseable que las series temporales sean estacionarias porque, de esta forma, es m谩s f谩cil hacer predicciones. Como la media es constante, esta puede predecirse usando todos los datos disponibles y usarse para predecir nuevas observaciones.
    - Series temporales no estacionarias: 
        - Cambios en la varianza
        - Son estacionales
        - Tienen tendencias. 
    - An谩lisis de series temporales: una serie temporal puede expresarse como la suma de varias componentes b谩sicas, que son la tendencia, la estacionalidad y un t茅rmino irregular o aleatorio. 
        - Componentes de una serie temporal no estacionaria: 
            - Tendencia: el cambio a largo plazo de la media
            - Estacionalidad: Pueden presentar estacionalidad. 
            - Aleatoriedad: ruido. 
        -Con el objeto de analizar la serie temporal y conocer su comportamiento a largo plazo, es necesario aislar de alguna manera la componente irregular para poder describirla con el modelo probabil铆stico m谩s adecuado. 
            - Enfoque descriptivo: se estiman tanto la tendencia como la estacionalidad, quedando solo la aleatoriedad. 
            - Enfoque Box-Jenkins: Usando transformaciones y filtros se eliminan tanto la tendencia como la estacionalidad de la serie temporal. 
        - Heterocedasticidad: que la varianza aumente de forma proporcional en el tiempo.
        - Estimaci贸n de la tendencia: 
            - Relaci贸n determinista: En algunas ocasiones se puede expresar la relaci贸n entre la tendencia y el tiempo de forma lineal. 
            - Tendencia evolutiva: consiste en suponer que la tendencia evoluciona de forma suave en el tiempo y que en los intervalos cortos de tiempo se puede aproximar medias m贸viles. 
                - Cuanto mayor sea el orden de la media m贸vil mayor ser谩 el suavizado, pero se perder谩n m谩s datos en el proceso. 
            - Diferenciaci贸n de la serie: Consiste en suponer que la tendencia evoluciona lentamente en el tiempo y no asumir ninguna forma determinada. 
        - Estimaci贸n de la estacionalidad: 
            - Coeficientes estacionales: se calculan como la media de las observaciones del periodo correspondiente, menos la media global de todo el periodo. 
                - Una vez calculado el coeficiente estacional, se desestacionaliza la serie restando a cada observaci贸n mensual su coeficiente estacional.
            - Diferenciaci贸n estacional: similar a la tendencia. 
    - Predicci贸n de series 
        - Es necesario tener muchos datos. 
        - Para predecir la componente estacional, normalmente se presupone que esta no
cambia en el tiempo o lo hace de forma muy lentamente, por lo que se toma el valor de la componente estimada para el a帽o anterior.
        - Modelo Box-Jenkins Arima
            - Son modelos de an谩lisis y predicci贸n 煤tiles para variables unidimensionales que dependen del tiempo. Estos modelos se basan en la hip贸tesis de que los datos son estacionarios, por tanto, en el caso en que la serie no lo sea, hay que efectuar antes el an谩lisis y las transformaciones necesarias para convertirla en aproximadamente estacionaria. Para ello se hace uso de las herramientas vistas para la tendencia y la estacionalidad.
        - modelo de alisado exponencial: Se basan en modelos param茅tricos deterministas que se ajustan a la evoluci贸n de la serie. Se les dan distintos pesos a las observaciones, de forma que las observaciones m谩s recientes tienen m谩s peso que las m谩s alejadas en el tiempo. Los pesos caen de forma exponencial a medida que las observaciones se hacen m谩s viejas, de ah铆 su nombre
