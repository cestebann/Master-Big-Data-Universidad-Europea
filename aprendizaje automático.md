
Profesor: David Díaz Vico

# Contenido del Curso

## Clases 

1. [ 1/03 - Introducción al aprendizaje automático](https://github.com/cestebann/Master-Big-Data-Universidad-Europea/blob/master/aprendizaje%20autom%C3%A1tico.md#1-103---introducci%C3%B3n-al-aprendizaje-autom%C3%A1tico)
2. [ 7/03 - Aplicaciones e ingeniería de características del aprendizaje automático](https://github.com/cestebann/Master-Big-Data-Universidad-Europea/blob/master/aprendizaje%20autom%C3%A1tico.md#2-703---aplicaciones-e-ingenier%C3%ADa-de-caracter%C3%ADsticas-del-aprendizaje-autom%C3%A1tico)
3. [14/03 - Aprendizaje supervisado: Modelos Lineales](https://github.com/cestebann/Master-Big-Data-Universidad-Europea/blob/master/aprendizaje%20autom%C3%A1tico.md#3-1403---aprendizaje-supervisado-modelos-lineales)
4. [28/03 - Aprendizaje supervisado: Máquinas de soporte, modelos kernel y Naive Bayes](https://github.com/cestebann/Master-Big-Data-Universidad-Europea/blob/master/aprendizaje%20autom%C3%A1tico.md#4-2803---aprendizaje-supervisado-m%C3%A1quinas-de-soporte-modelos-kernel-y-naive-bayes)
5. [11/04 - Aprendizaje supervisado: Redes neuronales](https://github.com/cestebann/Master-Big-Data-Universidad-Europea/blob/master/aprendizaje%20autom%C3%A1tico.md#5-1104---aprendizaje-supervisado-redes-neuronales)
6. [18/04 - Aprendizaje supervisado: Deep Learning](https://github.com/cestebann/Master-Big-Data-Universidad-Europea/blob/master/aprendizaje%20autom%C3%A1tico.md#6-1804---aprendizaje-supervisado-deep-learning)
7. [25/04 - Aprendizaje supervisado: Árboles de Decisión](https://github.com/cestebann/Master-Big-Data-Universidad-Europea/blob/master/aprendizaje%20autom%C3%A1tico.md#7-2504---aprendizaje-supervisado-%C3%A1rboles-de-decisi%C3%B3n)
8. [5/05 - Aprendizaje no supervisado](https://github.com/cestebann/Master-Big-Data-Universidad-Europea/blob/master/aprendizaje%20autom%C3%A1tico.md#8-505---aprendizaje-no-supervisado)
9. [9/05 - Métricas e interpretaibilidad](https://github.com/cestebann/Master-Big-Data-Universidad-Europea/blob/master/aprendizaje%20autom%C3%A1tico.md#9-905---m%C3%A9tricas-e-interpretaibilidad)
10. [17/05 - Optimización de hiperparámetros](https://github.com/cestebann/Master-Big-Data-Universidad-Europea/blob/master/aprendizaje%20autom%C3%A1tico.md#10-1705---optimizaci%C3%B3n-de-hiperpar%C3%A1metros)
11. [24/05 - Etica de la IA y repaso](https://github.com/cestebann/Master-Big-Data-Universidad-Europea/blob/master/aprendizaje%20autom%C3%A1tico.md#11-2405---etica-de-la-ia-y-repaso)


## Apuntes

- Unidad 1. Introducción al aprendizaje automático
    - Introducción al aprendizaje automático
    - Aplicación de la IA al entorno empresarial.
    - Principales proveedores de IA.
    - Procesos del aprendizaje automático
- Unidad 2. Aprendizaje Supervisado
    - Métodos de predicción lineal
    - Métodos simbólicos
    - Fundamentos de Deep Learning
    - Problemas del aprendizaje predictivo
- Unidad 3. Otras técnicas de Aprendizaje
    - No Supervisado - Agrupamiento
    - No Supervisado – Reducción de dimensiones
    - Sistemas de Recomendación
    - Aprendizaje de métricas y ordenaciones
- Unidad 4. Evaluación del aprendizaje
    - Conceptos de Evaluación.
    - Evaluación del aprendizaje supervisado
    - Cálculo de medidas de efectividad del aprendizaje supervisado
    - Shap Values
    - Evaluación del aprendizaje no supervisado
- Unidad 5. Análisis de textos y contenidos multimedia
    - Análisis semántico y sentimientos.
    - Clasificación de imágenes con ML ‘clásico’
    - Introducción a Pytorch.
    - Detección de imágenes con 
- Unidad 6. Ética de la Inteligencia Artificial
    - Directrices éticas para el desarrollo de modelos de IA
    - Detección de sesgos
    - Ejemplos de dilemas éticos


# Examen 
1. 20 preguntas tipo test. Contenido: PDFs.
2. Ejercicio de programación en un Jupyter. 

![](/img/aprendizaje_automatico/examen.png)

![](/img/aprendizaje_automatico/examen_2.png)


Coger un dataset, hacer el modelo y evaluarlo con train-test, cross-validation y hacer una búsqueda de metaparámetros. 

- Escoger RandomizedSearch sobre GridSearch
- Escoger CrossValidate sobre Train Test
- Conseguir los datos
- Hacer particiones
- CV
- Seleccionar modelo
- configurar como es debido
- Elegir una buena métrica (no nos va a pedir ninguna métrica). Por ejemplo Sacar scores sobre las predicciones. 

# 1. 1/03 - Introducción al aprendizaje automático

## Objetivos

- Automatizar tareas que normalmente realizan seres humanos
- Tradicionalmente con sistemas expertos (IA basada en conocimiento)
- Ahora con Aprendizaje Automático (IA basada en datos)

## Conceptos Básicos

- Datos (D)
- Algoritmo de entrenamiento (t) (el optimizador)
- Modelo (M)

t:D --> M

## Tipos de Aprendizaje Automático 

- Aprendizaje supervisado
    - Clasificación 
    - Regresión 
    - Clasificación /regresión ordinal 
    - Selección /transformación de variables
- Aprendizaje no supervisado 
    - Clustering 
    - Selección / transformación de variables
- Aprendizaje semi-supervisado 
- Aprendizaje por refuerzo (no entra)
- Aprendizaje motivacional (no entra)

### Según el modelo 
- Modelos paramétricos (con aprendizaje basado en modelos)
    - Lineales
    - No lineales
- Modelos no paramétricos (aprendizaje basado en instancias)
    - Vecinos 
    - Procesos gaussianos
- Modelos semi-paramétricos 
    - Árboles de decisión 
    - Métodos kernel 

### Según el algoritmo de entrenamiento 

- Directorios/batch (solución analítica)
- Iterativos/ online (solución numérica)

## Funciones de pérdida y métricas

- Funciones de pérdida
    - Se minimizan durante el entrenamiento del modelo 
    - Log-loss/cross-entropy para clasificación 
    - MSE para regresión 
- Métricas
    - Se aplican una vez entrenado el modelo para determinar si es útil para el negocio. 
    - Accuracy (precisión, recall, F1), AUC, para clasificación
    - MCE, MAE, R2, para regresión. 

- ¿por qué es alta la función de pérdidas?
    - Error de estimación (dato)
        - Datos escasos, incorrectos, missing values, datos marginales, maldición de la dimensionalidad (error por tener muchas variables), etc. 
    - Error de aproximación (modelo)
        - Modelo no suficientemente expresivo para el problema 
        - Modelo demasiado complejo para la cantidad de datos disponible.
    - Error de optimización (algoritmo de entrenamiento)
        - Entrenamiento insuficiente (underfit)
        - Entrenamiento excesivo (overfit)
        
## Metodologías
- NO hay que tomárselas como guías que hay que seguir al pie de la letra. 
- pretenden estandarizar la práctica del análisis de datos. Son orientativas, no siempre se siguen al pie de la letra. 
- Unas son más amplias que otras, pero en general coinciden en lo básico. 

Las más conocidas son: 
- KDD (Knowledge Discovery from Data)
- SEMMA (Sample, Explore, Modify, Model, Assessment)
- CRISP-DM (CRoss-Industry Standard Process for Data Mining)

![](/img/aprendizaje_automatico/metodologia.png)

## Principales proveedores de IA y casos de éxito 

- IBM 
- Amazon 
- Microsoft 
- Google 

### IBM 
- Cloud computing, computación cuántica, consultoría
- Watson
    - Responde a preguntas en lenguaje natural (DeepQA)
    - Adquisición de contenido de fuentes estructuradas y no estructuradas
    - Clasificación, detección de relación entre entidades. 
    - Generación, puntuación y ranking de hipótesis. 

### Amazon 
- Marketplace 
- Cloud Computing (AWS)
- Alexa 

### Microsoft 
- Sistema operativo, ofimática. 
- Cloud Computing (Azure)
- Cortana 

### Google 
- Buscador, correo, Maps, Android
- Cloud Computing (Google Cloud)
- Asistente Google

##  Herramientas 

### Python 
- Lenguaje de propósito general y alto nivel 
- Interpretado y con tipado dinámico, lo que agiliza el protipado y facilita el trabajo interactivo. 
- Tiene gran cantidad de librerías de código libre: 
    - NumPy
    - SciPy
    - Scikit-learn 

### Anaconda / Miniconda
- Distribución de Python más extendida

## Práctica

- KneighborClassifier(n_neighbors=3)


# 2. 7/03 - Aplicaciones e ingeniería de características del aprendizaje automático

### Aplicaciones

#### Sector financiero
- Asistentes virtuales (banca online)
- Valoración de riesgo de crédito 
- Detección de fraude y blanqueo de capitales
- Asesoría de inversiones, seguros, etc. 

#### Sector industrial
- Automatización de cadenas de producción 
- Coche autónomo 
- Gestión de stocks y demandas
- Ayuda en los diagnósticos


### Ingeniería de características

#### La maldición de la dimensionalidad
- Al aumentar la dimensionalidad de los datos, el volumen que ocupan crece exponencialmente y estos se hace más dispersos en el espacio, lo cual dificulta el entrenamiento de modelos de Aprendizaje Automático que generalicen correctamente

En la imagen de abajo vemos cómo cambia el espacio muestral cuando aumentamos el número de variables explicativas. Como consecuencia nuestros modelos pueden predecir erróneamente.

![](/img/aprendizaje_automatico/aumento_dimensionalidad.png)

#### Principio de Parsimonia / navaja de Okham 

"En igualdad de condiciones, la expllicación más sencilla suele ser la más probable". 

En Aprendizaje Automático: "cuantas menos variables utilice el modelo (y por lo tanto menos hiperparámetros), mejor."

#### Mejora en las predicciones

Los resultados con un mismo modelo y algoritmo de entrenamiento pueden mejorar mucho si se utilizan las características correctas. 

Por ejemplo , un modelo lineal puede ser capaz de resolver problemas no lineales si se le suministran las características apropiadas, construidas manualmente como funciones no lineales de las características originales. 

#### Selección de variables

Se trata de seleccionar el subconjunto de variables que sea más adecuado para resolver el problema. 

Algunas técnicas habituales son: 
- Eliminación de variables con poca varianza. 
- Uso de tests estadísticos univariantes (chi cuadrado)
- Eliminación recursiva de variables, métodos wrapper, etc.  (es un método muy costoso porque selecciona las mejores variables para producir el modelo más efectivo al probar todas las variables).

``` python 

from sklearn.datasets import make.classification 
```

#### Transformación de variables
En este caso en lugar de seleecionar un subconjunto de variables se generan variables nuevas, "sintéticas", a partir de las originales, esperando que sean más útiles para resolver el problema. 

La técnica general más habitual es el análisis de componentes principales (PCA), aunque hay muchos métodos específicos para transformación de texto, imágenes, etc. 

- Principal Component Analysis: Realiza una proyección de los datos originales en un espacio de dimensión menor en el que se maximiza la varianza. Las nuevas variables son combinaciones lineales de las originales. No utiliza la variable target, es una técnica no supervisada.

- Linear Discriminant Analysis (solo clasificación): Realiza una proyección de los datos originales en un espacio de dimensión menor en el que se maximiza la separabilidad. Las nuevas variables son combinaciones lineales de las originales. 

![](/img/aprendizaje_automatico/lda.png)


### Práctica 


# 3. 14/03 - Aprendizaje supervisado: Modelos Lineales


## Modelos Lineales

![](/img/aprendizaje_automatico/modelo_lineal.png)

Los modelos lineales funcionan muy bien para predecir variables que se pueden modelizar con una variable explicativa. 


### Ventajas
- Eficaces (resuelven problemas usando pocas variables paramétricas)
    - Los modelos lineales son el claro ejemplo de un modelo paramétrico: son modelos que al entrenarse guardan información del proceso en variables internas, y al momento de predecir sobre unas variables desconocidas utilizan esos parámetros. 
- Rápidos al predecir
- Interpretalbes (un parámetro pro cada variable de entrada indicando su relevancia).
- Estables (cambios pequeños en las variables de entrada suponen cambios pequeños en los parámetros al entrenar.)

### Desventajas
- Poder expresivo limitado por su naturaleza lineal. 

### Modelos Lineales: Regresión Lineal 

![](/img/aprendizaje_automatico/regresion_lineal.png)


y = variable a predecir
y con gorro = predicción del modelo. 
x= variable explicativa o independiente.
m = parámetro interno del modelo que se perfecciona durante el entrenamiento
b = parámetro interno del modelo que se llama sesgo (bias) o intercepto. 

Hipótesis
- Linealidad: La variable respuesta es combinación lineal de las variables independientes. 
- Ausencia de multicolinealidad: no hay colinealidad entre las variables independientes. 
- Homocedasticidad: la varianza de los residuos (errores) es constante entre observaciones. 
- Ausencia de autocorrelación en los errores: los errores no están correlacionados entre observaciones. 

#### Solución directa 

Ordinary Least Squares:Estamos buscando el parámetro que dé el mínimo error. 

![](/img/aprendizaje_automatico/se.png)

En esta solución no puede haber dependencias lineales entre las variables (hipótesis 2), porque de lo contrario no podremos calcular la función inversa. 

Este modelo no podría aplicarse para datasets grandes. 


##### ¿Qué quiere decir que el sistema sea capaz de aprender?
- No debería necesitar que un humano codifique el conocimiento.
- Debería aprender únicamente observando los datos.
- El aprendizaje debería ser acumulativo. 


#### Solución iterativa: solución aproximada

![](/img/aprendizaje_automatico/solucion_iterativa.png)


#### Regularización 

![](/img/aprendizaje_automatico/regularizacion.png)


![](/img/aprendizaje_automatico/regularizacion_2.png)

peso=parámetros

##### Variables categóricas
- Los modelos lineales funcionan únicamente con datos numéricos. 
- Las variables categóricas deben ser transformadas a codificación one-hot (codifica cada categoría como una combinación de 0s y 1s.)


### Modelos Lineales: Regresión logística

![](/img/aprendizaje_automatico/regresion_logistica.png)

![](/img/aprendizaje_automatico/regresion_logistica_2.png)

![](/img/aprendizaje_automatico/regresion_logistica_3.png)

## Práctica 1

- Es un problema de clasificación binaria, donde debeos hacer un modelo de supervisión de clasificación binaria (debe clasificar a los clientes entre propensos a contrar y no propensos a contatar.)

``` bash
conda env create -f <nombre del fichero>
# luego de crear el entorno con sus dependencias lo activamos
conda activate <nombre del entorno>
#ahora vamos a utilizar jupyter notebooks dentro del entorno, para ello aplicamos este código
python -m ipykernel install --user --name nombre_del_kernel --display-name "Nombre del kernel"
#Reemplaza nombre_del_kernel con el nombre que deseas dar al kernel de Python dentro del entorno virtual y Nombre del kernel con el nombre que deseas que aparezca en Jupyter Notebook.
#por último ya ejecutamos jupyter notebook 
jupyter notebook
```

# 4. 28/03 - Aprendizaje supervisado: Máquinas de soporte, modelos kernel y Naive Bayes

## Máquinas de Vector De Soporte

### Linear SVC (Support Vector Classifier)

Más que cambiar el modelo, lo que hace este modelo es cambiar la forma en que se entrena. 

A partir de ahora siempre vamos a estar hablando de métodos iterativos (ya que el método directo es muy costoso.)

![](/img/aprendizaje_automatico/linear_svc.png)

La idea de este modelo es ser estricto a la hora de escoger cuál solución nos quedamos, y es donde entra el concepto de margen, y aquí es donde entramos a definri el margen de separación. 

![](/img/aprendizaje_automatico/linear_svc_2.png)

La recta H1 es mala. En cambio, las rectas H2 y H3 clasifican perfectamente el dataset. Pero el H3 tiene un margen de separación más grande que H2, que están deterinados por los vectores de soporte (la distancia mínima a un punto). 

De acuerdo a este modelo, la recta H3 es la solución más robusta ya que cuando incorporemos datos de test, es probable que se acerquen a la frontera y el modelo que tenga el mayor margen de separación es el que menos errores va a cometer y por ende va a cometer menos errores de clasificación. 

![](/img/aprendizaje_automatico/linear_svc_3.png)

![](/img/aprendizaje_automatico/linear_svc_4.png)

![](/img/aprendizaje_automatico/linear_svc_5.png)

Es como una regresión logística pero con una función de coste distinta. Gracias a ello conseguimos un modelo lineal con un margen de separación máximo. 


### Linear SVR (Support Vector Regrresion)

En este modelo queremos conseguir que todos los puntos estén contenidos en un ancho de tubo estrecho. 

![](/img/aprendizaje_automatico/svr_1.png)
![](/img/aprendizaje_automatico/svr_2.png)


## Modelos Kernel

- Los modelos lineales tienen muchas virtudes, pero su expresividad está muy limitada por su naturaleza lineal. 

- Por suerte, existe un truco matemático para hacer questos modelos funcionen en un espacio de dimensiónn muy alto o incluso infinito, en el cual los problemas de clasificación son siempre linealmente separables y los problemas de regresión siempre se pueden resolver con una forma lineal, evitando tener que realizar operaciones matemáticas extremadamente costosas en dicho espacio de alta dimensión. Es el llamado "truco del kernel". 
- Para ello, es necesario que el modelo pueda expresarse en términos de x=xxt en lugar de Z. Las SVM cumplen esta condición. 
- Sin embargo, estos métodos tienen un coste de entrenamiento y de predicción muy superior al de sus contrapartidas lineales, y su uso está limitado a datasets pequeños. 

![](/img/aprendizaje_automatico/kernel.png)

Este es un modelo bastante potente pero no es óptimo para datasets grandes ya que puede ser muy lento. A raíz de esto, han surgido las redes neuronales. No obstante, este no deja de ser es un modelo muy potente. 


## Modelos Naive Bayes

Este es un modelo que en la práctica no se suele utilizar y funciona estrictamente para problemas de clasificación. 

![](/img/aprendizaje_automatico/naive_bayes.png)
![](/img/aprendizaje_automatico/naive_bayes_2.png)


### Práctica 
https://ml-playground.com/


# 5. 11/04 - Aprendizaje supervisado: Redes neuronales

### Limitaciones del modelo lineal

Problemas no linealmente separables: Si el problema no es linealmente separable, un modelo lineal no es suficientemente expresivo como para resolverlo.

Las redes neuronales resuelven esta limitación que tienen los modelos lineales.

### Componentes del error

1. Estimación: determinado por el tamaño y la calidad de los datos. 
2. Aproximación: determinado por la expresividad del modelo.
3. Optimización: determinado por el método de entrenamiento. 

Ahora mismo, los modelos lineales tienen problemas de aproximación que las redes neuronales sí son capaces de resolver. 

### Inspiración biológica

> Red neuronal artificial: “Paradigma de aprendizaje y procesamiento automático inspirado en la forma en que
funciona el sistema nervioso biológico. Se trata de un sistema de interconexión de neuronas
que colaboran entre sí para producir un estímulo de salida.”

![](/img/aprendizaje_automatico/neurona_naturall.png)

![](/img/aprendizaje_automatico/neurona_artificial.png)

![](/img/aprendizaje_automatico/neurona_artificial_2.png)

![](/img/aprendizaje_automatico/neurona_artificial_3.png)

![](/img/aprendizaje_automatico/neurona_artificial_4.png)

Como podemos ver, el perceptrón es un modelo lineal. No hay nada nuevo hasta ahora. 
La parte innovadora no es en el concepto del perceptrón, sino en el conjunto de perceptrones. El modelo que se utiliza actualmente es el de perceptrón multicapa. 
En la imagen tenemos 3 variables de entrada (círculos rojos, no son neuronas).
Tenemos 4 neuronas o 4 unidades en una capa oculta y una capa de salida de dos neuronas, tenemos una capa de dos dimensiones. 
Solo podemos conectar cada capa con la siguiente. 

- X = variables de entrada
- W1 = pesos (hiperparámetros) de las neuronas de la capa oculta
- W2 = pesos de las neuronas de la capa de salida. 
- F1 = Función de activación de la capa oculta. 
- F2 = Función de activación de la capa de salida

¿por qué ya no nos vale el método del descenso del gradiente? 


![](/img/aprendizaje_automatico/neurona_artificial_5.png)

#### Back-propagation

Tenemos una fase hacia adelante donde calculamos la activación desde la entrada hasta la salida. 
A partir de eso, calculamos la activación de la salida. 

### Teorema de Aproximación Universal

![](/img/aprendizaje_automatico/teorema_aproximacion_universal.png)

Este teorema nos dice qué se puede resolver con una red neuronal y qué no se puede resolver con una red neuronal. 

### El perceptrón Multicapa

- Cuanto más ancho y profundo es un perceptrón multicapa, más expresivo es. 
- Vamos a verlo con más detalle con algunos problemas de clasificación binaria en el plano. 

### Dificultades

- Mínimos locales más frecuentes cuantas más capas y unidades tiene el modelo
- Sobreajuste si el modelo es demasiado expresivo y no hay muchos datos

![](/img/aprendizaje_automatico/redes_neuronales.png)


El early stopping nos dice el número máximo de iteraciones. 
Una iteración es una época. 
Los datos de validación evalúan el error por cada época o iteración. 

![](/img/aprendizaje_automatico/early_stopping.png)

# 6. 18/04 - Aprendizaje supervisado: Deep Learning

![](/img/aprendizaje_automatico/representation_learning.png)

El Deep Learning consiste en redes neuronales con muchas capas. Básicamente, los modelos de Deep learning van creando representaciones de los datos, una tras otra cada vez más sofisticadas, cada una a partir de la anterior.
Y llega un momento que la representación es tan buena que el problema ya es muy fácil de resolver. Lo que pretenden es ahorrar buena parte (no todo, porque siempre hay que siempre hay que preparar un poco los datos), del ETL (del procesamiento previo de los datos antes de de alimentar con ellos al modelo predictivo), pues tradicionalmente se ha hecho de forma manual, lo que es el Feature Engineering. Pues esto con el Deep learning.



#### Representation Learning 

![](/img/aprendizaje_automatico/representation_learning_2.png)


#### ¿Por qué no añadir más y más capas para conseguir mejores resultados?

A medida que pasa por más capas de la red neuronal, la información mejora, pero con 3 o 4 capas comienzan a surgir problemas.

##### Vanishing Gradient
- Desvanecimiento del gradiente. El gradiente se refiere al gradiente de la función de error respecto a los parámetros de la red, que es lo que en el método de back-propagation se va calculando capa a capa desde la salida hacia la entrada.
- Se observó es que este gradiente en las capas más cercanas a la salida de la red tenía un módulo grande al comienzo pero al entrenar el modelo de iteración a iteración, a medida que se iba propagando a través de las capas hacia la entrada de la red, el gradiente sigue haciendo cada vez más pequeño: se iba desvaneciendo el gradiente.
Durante el back-propagation entonces,  si el gradiente es muy pequeño, las capas no se entrenan porque los valores de sus parámetros apenas varían de iteración en iteración. 


###### Causas del vanishing gradient 

1. EL uso de funciones de pérdidas inapropiadas: en regresión, MSE y en clasificación cross-entropy. Aunque es el menor de los problemas porque hoy en día se sabe qué función de pérdida se debe utilizar para el tipo de modelo de aprendizaje (predicción o clasificación)
2. Función de activación escogida: 
    - Las funciones de activación tradicionales están acotadas y se saturan fácilmente. 
    - Esto hace que al apilar muchas capas el gradiente de la función de error tiende a 0. Las capas ya no pueden entrenar más.
    - Si el gradiente es muy pequeño, el entrenamiento es muy lento. 
    - Uno de los problemas es la función de activación que se estaba usando que era la función sigmoide (tiene asíntotas horizontales en los extremos)
    - Por lo tanto, se introdujo una nueva función que se llama ReLU, que es muy fácil de computar. Como beneficio es que podemos tener más ciclos de entrenamiento. 
3. Inicialización de los pesos de las neuronas: 
    - Tradicionalmente en una red neuronal los pesos se inicializan aleatoriamente, lo que se hacía para generar esos valores aleatorios iniciales era una distribución uniforme en un intervalo que venía determinado por el número de neuronas que tenía cada capa. Por ejemplo, si estoy inicializando una capa de 10 neuronas, pues levanto una distribución uniforme con los intervalos definidos en función de esas 10 unidades.
    - Glorot y Bengio demostraron en el paper *Understanding the difficulty of training deep feedforward neural networks*" que para conseguir que el gradiente de la función de error, se mantenga en unas magnitudes más o menos constantes, capa a capa, hay que tener en cuenta no solo el tamaño de la propia capa que se está inicializando, sino también el tamaño de la siguiente capa (aquella donde le llega el error) y además, la función de activación que se usa en las capas. En función de ya no solo las unidades de la capa, sino en función de estas 3 cosas, la incialización del back propagation funciona muchísimo mejor. 


![Función relu](/img/aprendizaje_automatico/relu.png)


![Función sigmoide](/img/aprendizaje_automatico/funcion_sigmoide.png)


Cuando se abordan estos 3 puntos el vanishing gradient deja de ser un problema. No obstante, cuando tenemos una gran cantidad de datos, otro problema puede surgir. 


##### Internal Covariate Shift
- Los modelos lineales y las redes neuronales necesitan entradas estandarizadas para entrenarse correctamente con métodos basados en descenso por gradiente. 
- Las capas ocultas reciben las activaciones de las capas previas no estandarizadas
    - Si estandarizas las variables te quitas el problema en la primera capa pero la primera capa transforma los datos con una función de activación que devuelve una salidas con rangos muy heterogéneos: unas serán muy grandes, otras serán muy pequeñas.
    - Estandarizar la variable solo te resuelve el problema en el primer nivel. 
- Esto produce los problemas de convergencia que teníamos al entrenar una regresión sin variables estandarizadas.

**¿Cómo se debe resolver esto?**

Se hace una estandarización, capa por capa por por mini-batches durante el entrenamiento, o sea, las activaciones de cada capa se estandarizan con cada mini batch de datos. Esta es una técnica que en general no suele hacer falta, pero puede ser un problema en una red neuronal muy grande.

##### Recomendaciones al crear un modelo de Deep Learning: inicalización, activación y regulación

- Se debe inicializar los pesos con una distribución que produzca gradientes del error
grandes y de la misma magnitud en todas las capas
- Hay que utilizar funciones de activación que no saturen tan fácilmente como las
clásicas
- Se deben utilizar técnicas de regularización basadas en ruido y normalizar batches
de datos entre las capas ocultas (batch-normalization para evitar el internal covariate shift y también técnicas basadas en ruido para evitar sobreajustes, como el dropout, que es una técnica de regularización que desactiva aleatoriamente durante el entrenamiento unidades en la capa. Como consecuencia, el modelo tiene el entrenamiento más complicado para evitar el sobreajuste y que cada una de las neuronas sean "útiles" sin confiar  que las demás vayan a serlo, ya que si desactivamos una neuronas, el modelo debe ser consistente para procesar la información con la la misma calidad de aprendizaje que con las neuronas activadas). 


![](/img/aprendizaje_automatico/convolucion.png)


### Datos espaciales y redes convolucionales 

Las redes convolucionales están adaptados para una estructura espacial o en forma de rejilla (imágenes por ejemplo). El orden de las columnas de la matriz sí importa. 

Un modelo clásico no entiende que pueda haber un orden en las columnas (una neurona simple, un árbol de decisión). Le da igual que las columnas vengan en un orden u otro. 

![](/img/aprendizaje_automatico/redes_convolucionales.png)

![](/img/aprendizaje_automatico/convolucionales-2.png)

Ejemplo de una red convolucional

![](/img/aprendizaje_automatico/convolucionales-3.png)

Las redes neuronales no solo aplican para imágenes o videos sino también para cualquier dato que esté en forma de rejilla (datos meteorológicos)

### Datos temporales y redes recurrentes 

Los datos temporales pueden ser los datos bursátiles, donde las filas tienen un orden cronológico por timestamp. 

¿Cómo consiguen la memoria las neuronas recurrentes para saber que lo que están recibiendo son datos con estructura temporal? Bueno, pues la forma básica de neuronas recurrente es simplemente coger una neurona y hacerle una, una conexión de realimentación.

Le da a la neurona una memoria muy corta. 

![](/img/aprendizaje_automatico/redes_recurrentes_2.png)

Ejemplos de información que funcionan bien con redes recurrentes son: videos, lenguaje natural, etc. 

Para más información 
http://colah.github.io/posts/2015-08-Understanding-LSTMs/


![](/img/aprendizaje_automatico/redes_recurrentes.png)

###  Transformers y NLP

![](/img/aprendizaje_automatico/nlp_transformer.png)

### Resumen 

Funciona bien en los problelmas en los que: 
- Los datos tienen estructura espacial o temporal. 
- El conjunto de datos es grande. 

Ejemplos: 
- Imágenes (convolucional)
- Video (convolucional y recurrente)
- Procesamietno de voz (recurrente)
- Lenguaje natural (recurrente o transformer)
- Series temporales (recurrente)

### Librerías

- Keras (es como Scikit-learn pero más complicado)
- Torch / PyTorch
- Las GPU ayudan mucho 



# 7. 25/04 - Aprendizaje supervisado: Árboles de Decisión 

#### Métodos simbólicos

- La corriente dominante de la IA hasta el auge del Aprendizaje Automático fue la de los Métodos Simbólicos.
- Los Sistemas Expertos forman parte de esta rama de la IA.
- Hoy en día son menos populares que el Aprendizaje Automático, pero siguen siendo muy utilizados y en muchas ocasiones complementan a los modelos de Aprendizaje Automático.
- Producen modelos fácilmente comprensibles e interpretables gracias a su manipulación explícita de las variables.
- La principal limitación de los Sistemas Expertos es el enorme coste que tiene la generación de su base de datos de reglas de conocimiento. Esto llevó al desarrollo de algoritmos de aprendizaje de reglas buscando al automatización de dicho proceso.
- Existen numerosos algoritmos de aprendizaje de reglas. PRISM es un ejemplo.


#### PRISM 

- PRISM (Programming In Statistical Modeling) es uno de los algoritmos de aprendizaje de reglas más conocidos.
- En cada paso construye una regla con un test sobre una variable que cubre un conjunto de ejemplares y se mide su nivel de acierto para el target. Se guarda la regla con mejor nivel de acierto. Se continua el proceso iterativo con los ejemplares cubiertos por la regla. El proceso termina al conseguir reglas con 100% de acierto para todos los ejemplares o cuando ya no hay más variables que utilizar.
- La evolución de este tipo de procedimientos da lugar a los modelos simbólicos de Aprendizaje Automático. En particular los Árboles de Decisión.

### Árbol de decisión 

- Son modelos de caja blanca (muy fáciles de interpretar, junto con los modelos lineales y de K-vecinos.)
- Son modelos de Aprendizaje Automático en los que las variables se manipulan de forma explícita, siendo los más fácilmente interpretables y sencillos de comprender junto con los modelos lineales y de vecinos.
- Los Árboles de Decisión son una evolución de los Sistemas Expertos y los algoritmos de Aprendizaje de Reglas. Los Árboles de Decisión construyen un conjunto de reglas de manera automática a partir de los datos etiquetados.

![](/img/aprendizaje_automatico/arbol_decision.png)

En este diagrama podemos apreciar que tiene una profundidad de 4 ramas, las ramas son los recuadros con bifurcaciones. Además, tenemos las hojas que son los recuadros al final de de las ramas. Sumando todas las hojas, debe resultar el tamaño del dataset utilizado 

#### Parámetros

- **Profundidad:** El número de ramas que puede tener el árbol. Habitualmente, se usa una profundidad de 3-6. El número de hojas va a a ser 2^n, siendo n la profundiad. 
- Criterio de calidad de los cortes
- Estrategia para realizar el corte
- Número mínimo de elementos por hoja
- Número mínimo de elementos para cortar

![](/img/aprendizaje_automatico/arbol_decision_2.png)

Las fronteras de clasificación utilizando el modelo de árbol de decisión son perpendiculares a los ejes y tiene sentido porque estamos cortando las variables. 

![](/img/aprendizaje_automatico/arbol_decision_3.png)

### Ensembles

Un solo árbol de decisión, por lo general no son muy potentes. Existen formas de mejorar un modelo de machine learning, que es construir *ensembles*: combinar modelos que ayudan a obtener mejores resultados. Aplica a los árboles de decisiones especialmente, porque si les cortamos la profundiad, son muy robustos y ajustan muy poco: son weak learners. 


Técnicas de combinación de modelos (ensembles)
• Bagging (Bootstrap aggregating)
• Boosting
• Stacking
• Otras

#### Bagging (Bootstrap aggregating)

Es una forma de construir ensembles. Una característica que tenemos que asegurar en los modelos es que exista diversidad: los modelos deben ser muy distintos entre ellos, porque así mejoramos la predicición de los datos. 

- En bagging, la diversidad la conseguimos particionando el dataset. 
- Utilizamos varios árboles de decisión y les damos a cada uno una partición del dataset.
-  Se entrenan por separado y en paralelo. 
- Cada árbol emite su voto de predicción. 
- Se elige como predicción global la clase mayoritaria más votada. 
- Como resultado, reduce la varianza de las predicciones. 
- El modelo más popular es el Random Forest. Funciona excelentemente para datos tabulares. No es un bagging simple, agrega una característica mayor a la diversidad: No solo reparte una partición del dataset a cada variable, sino que también discrimina las variables y las reparte a los diferentes árboles de decisión. De esa manera diversifica la capacidad de predicción de los modelos. 


#### Boosting 

- Cada modelo se entrena dando más importancia al subconjunto de los datos que obtuvo malas predicciones con los modelos entrenados anteriormente. 
    - Una vez entrenado el árbol de decisión, se pasa sobre el dataset, se generan las predicciones, se calculan las métricas de de los scores.
    - Y los datos para los que las métricas son peores se les da más importancia en el dataset para la siguiente iteración: se sobremuestrean.
    - Tenemos un nuevo dataset en el que están los datos originales, pero además los datos para los que la predicción del modelo entrenado en primer lugar no fue muy buena, esos aparecen más veces y tienen más peso claro, van a tener más influencia en el entrenamiento del siguiente modelo. 
    - Se entrena un segundo árbol de decisión y esta vez con el data SET ya modificado. Entonces, ese nuevo rol de decisión va a aprender a hacer mejores predicciones en los datos en los que el primero falló porque esos datos aparecen más durante su entrenamiento.
    - Añadimos este segundo árbol de decisión al ensemble que ya teníamos con el primer árbol de decisión, lo aplicamos al dataset de entrenamiento, y otra vez habrá datos con buenas predicciones y malas predicciones. 
    - Los datos con malas predicciones se vuelven a sobremuestrear y así se entrena un tercer modelo y se añaden al ensemble, y así sucesivamente. 
    - En resumen, se agrega un nuevo modelo en cada iteración, y el nuevo modelo se enfoca en los datos para los que no se predice bien en el ensemble hasta ese momento. 
-  Entrenamiento secuencial, no paralelizable. 
- Reduce el sesgo de las predicciones
- Los ejemplos más conocidos son AdaBoost(Scikit-learn), Gradient Boosting (Scikit-learn) y Extreme Gradient Boosting (**es excelente para datos tabulares, junto con los random forests**).

![](/img/aprendizaje_automatico/bagging.png)


#### Stacking

- No es muy utilizado. 
- Se entrena un modelo para combinar las predicciones de un conjunto heterogéneo de modelos (una red neuronal, un árbol de decisión, máquina de soporte, un random forest, etc.)




# 8. 5/05 - Aprendizaje no supervisado

Práctica 2
- Hacer un estudio con técnicas no supervisadas para agrupar los clientes en distintas categorías y luego qué se puede hacer con ello. 
- Ser rigurosos con la metodología. 


Práctica 3. Reconocimiento de dígitos manuscritos

- Hacer una mejora en el código. 

En este caso el target (y) no está disponible como en el caso del Aprendizaje Supervisado.
Veremos dos tipos de técnicas de Aprendizaje No Supervisado:

- Clustering: El objetivo es agrupar los datos. 
- Reducción dimensialidad. 

## Clustering 

El objetivo es agrupar los datos en un número prefijado de categorías o “clusters” que tengan sentido. 

Aunque el target no está disponible, y por tanto no hay una “respuesta correcta”, sí hay una serie de métricas que permiten estimar la bondad de los clusters obtenidos.


![](/img/aprendizaje_automatico/clustering.png)

La inercia es equivalente al MSE en regresión pero extrapolado a la clasificación. La inercia es la suma de la distancia de los puntos y los puntos del centroide del clúster asignado a esos puntos. Esa distancia se llama residuos. En pocas palabras, la inercia es la suma de los residuos. Cuanto menor sea la inercia, más concentrados estarán los puntos. 

### K-means

Una vez elegido el número de clusters K y el umbral mínimo de su loss function (normalmente inertia), sigue un proceso iterativo:

-  Elige aleatoriamente K centroides entre las muestras
- Itera los siguientes pasos:
- Asigna a cada muestra el centroide más cercano
-  Crea nuevos centroides como la media de las muestras asignadas a cada cluster
-  Si la loss cae debajo del umbral, termina

https://youtu.be/2kfY0R34Dy0?t=156

- No funciona bien cuando hay clusters de tamaños muy distintos
- Es muy inestable y sensible a outliers
- Su resultado depende de la inicialización

![](/img/aprendizaje_automatico/k_means.png)

![](/img/aprendizaje_automatico/k_means_2.png)

![](/img/aprendizaje_automatico/k_means_3.png)

![](/img/aprendizaje_automatico/k_means_4.png)

![](/img/aprendizaje_automatico/k_means_5.png

### Hierarchical Clustering


-  Es una familia de métodos de clustering que funcionan generando una jerarquía de clusters anidados que dividen o unifican
sucesivamente
- Tipo aglomerativo: enfoque bottom-up, empieza con un cluster por cada dato y va fusionando clusters
- Tipo divisivo: enfoque top-down, empieza con un cluster para todos los datos y va dividiendo clusters


### Gaussian Mixture

- Supone que los datos están generados por una mezcla de un número finito de distribuciones gaussianas con parámetros
desconocidos. Cada una de esas gaussianas se corresponde con un cluster.
- Es parecido al modelo K-Means, pero además de los centroides estima también las covarianzas de los datos.
- Se entrena con el algoritmo Expectation-Maximization (EM)

https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Examples

![](/img/aprendizaje_automatico/comparacion.png)

## Reducción de Dimensionalidad

### Principal Component Analysis 

![](/img/aprendizaje_automatico/pca.png)


### Kernel Principal Component Analysis 

- PCA se expresa en términos de XtX, por tanto se puede utilizar el truco del kernel para aumentar su expresividad y conseguir transformaciones no lineales.
- Al igual que con otros métodos kernel, existen una gran variedad opciones como kernel, pero la más habitual es de nuevo el kernel RBF o gaussiano.
- El uso de kernel confiere a PCA mucha más expresividad, a cambio de un coste computacional mayor en entrenamiento e inferencia.

### Indpendent Component Analysis 

- También es un método de transformación lineal
- En este caso, el criterio es el de minimizar la información mutua entre las distintas componentes generadas
- Su uso más habitual es el de separar señales, por ejemplo en una grabación de audio en la que se oye a varias personas hablar, se podría utilizar ICA para obtener por separado las señales de audio de cada persona.



# 9. 9/05 - Métricas e interpretaibilidad

## Métricas e interpretabilidad 

La selección de la métrica es una de las decisiones más importantes y primeras que se deben hacer .

### Métricas 

- Ayudan a entender si el modelo entrenado será de utilidad en la práctica.
-  Se aplican una vez el modelo está entrenado, ya sea a una partición de test o haciendo validación cruzada.
- Suelen ser más fácilmente interpretables que las funciones de pérdida que se usan para entrenar los modelos. A veces una función de pérdida se puede utilizar también como métrica (MSE, por ejemplo).
- Elegir una métrica apropiada puede tener gran importancia para conseguir el modelo más adecuado para un problema dado. Por ejemplo, cuando se trabaja con un problema de clasificación muy desbalanceada o cuando un error de clasificación puede tener mayor o menor gravedad dependiendo de la clase.

#### Clasificación 

- **Eficacia del modelo:** cómo de bueno es realizando predicciones.
- **Eficiencia del modelo:** su velocidad a la hora de entrenarse o predecir, el coste computacional y de almacenamiento, etc.
- **Compacidad y expresividad del modelo:** la cantidad de parámetros que tiene (Principio de Parsimonia/Navaja de Okham),las hipótesis que hace sobre los datos, la capacidad expresiva que tiene, etc.
- **Claridad e inteligibilidad del modelo:** lo fácil que es entenderlo e interpretar sus predicciones (caja blanca, caja negra). 

#### Tipos 

##### Regresión 

![](/img/aprendizaje_automatico/metricas_regresion.png)


![](/img/aprendizaje_automatico/metricas_regresion_2.png)

##### Clasificación

![](/img/aprendizaje_automatico/metrica_clasificacion.png)


- Matriz de confusión: Además de indicar los aciertos y errores, indica en qué clases es más propenso el modelo a fallar. 
- Curva ROC
- AUC: Es una muy buena métrica para clasificación desbalanceada. Es mejor que *accuracy*. Por defecto, está definida para clasificación binaria. 


#### Clustering 

![](/img/aprendizaje_automatico/metricas_clustering.png)


### Interpretabilidad 

- Algunos modelos como los lineales o los árboles de decisión son fáciles de interpretar y es sencillo determinar qué peso ha tenido cada variable en sus predicciones.
- Sin embargo, en modelos como los métodos kernel o las redes neuronales, esto no es así. Estos modelos se llaman por ello modelos “de caja negra”.
- En algunas aplicaciones, esta dificultad para interpretar los modelos y sus predicciones hace que los modelos de caja negra deban ser descartados, a pesar de que puedan ofrecer mejores resultados que modelos más fácilmente interpretables.
- En los últimos años se han desarrollado algunas técnicas para poder entender mejor el funcionamiento interno de los modelos de caja negra y atribuir “pesos” a las variables de entrada a la hora de hacer predicciones.

#### SHAP Values


- Es una técnica de Teoría de Juegos, y permite explicar las predicciones de un modelo de Aprendizaje Automático midiendo la contribución marginal de cada una de las variables en la predicción.

https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html#Explaining-a-linear-regression-model

- LIME (https://github.com/marcotcr/lime)
- ELI5 (https://eli5.readthedocs.io)

# 10. 17/05 - Optimización de hiperparámetros

Práctica de la red convolucional

- Scikit-learn
- keras
- Tensor Flow

¿Cómo elegir los mejores meta-parámetros para cada modelo?

Normalmente no se sabe que meta-parámetros van a producir el mejor modelo, así que se prueban distintas combinaciones (habitualmente no se pueden probar todas) y se elige la mejor.

## Técnica de búsqueda de meta-parámetros 
- Rejilla
- Aleatoria
- Bayesianas
- Otras

Se suele usar CV para evaluar cada una de las combinaciones de meta-parámetros. 

## Búsqueda en rejilla (GridSearchCV)

- Se especifican los posibles valores que puede tomar cada meta-parámetro.
- Se prueban todas las combinaciones posibles.
- Para más de 2 ó 3 meta-parámetros suele ser poco práctico.
- Paralelizable (con varios CPUs o un clúster en HDFS o Spark)

## Búsqueda aleatoria (RandomizedSearchCV)

- Se especifican las distribuciones de probabilidad de cada meta-parámetro
- Se prueba un número fijo de combinaciones
- Es la opción más utilizada cuando la búsqueda en rejilla no es posible
- Paralelizable

![](/img/aprendizaje_automatico/metaparametros.png)

## Búsqueda Bayesiana (BayesSearchCV)

- Se especifican las distribuciones de probabilidad de cada meta-parámetro
- Se prueba un número fijo de combinaciones
- Un modelo predictivo (normalmente un proceso gaussiano) guía la búsqueda hacia regiones donde los resultados son mejores
- Es la opción más costosa y no es paralelizable, pero es muy interesante para modelos complejos

![](/img/aprendizaje_automatico/bayesSearch.png)

Voy probando con varios parámetros, entreno el modelo con esa configuración y guardo el score. A partir de esta información construimos un "metamodelo" regresor que nos permite predecir los valores del score a partir de una configuración de parámetros. Por lo tanto, cuando se seleccionan los metaparámetros se predice el score, si es muy bajo, entonces no se utiliza para el entrenamiento y se descarta. 


Cross-Val anidado vs. No anidado 

https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html#sphx-glr-auto-examples-model-selection-plot-nested-cross-validation-iris-py


Para abrir el notebook es necesario descargar: 


´´´ bash
scikit-optimize con conda forge 
´´´

## Otras

- Particle Swarm 
- Algoritmos Genéticos y Estrategias Evolutivas (https://rednuht.org/genetic_cars_2/)

# 11. 24/05 - Etica de la IA y repaso

## Principios éticos

- **Respeto a la autonomía humana:** los sistemas de IA no deben subordinar, coaccionar, engañar, manipular...
injustificadamente a humanos.
- **Prevención de daños:** los sistemas de IA no deben causar ni agravar los daños ni afectar negativamente a los seres humanos.
- **Equidad:** se debe garantizar una distribución equitativa y justa tanto de los beneficios como de los costes, y asegurar que los
individuos y los grupos estén libres de prejuicios injustos, discriminación y estigmatización.
- **Explicabilidad:** los procesos deben ser transparentes, las capacidades y el propósito de los sistemas de IA deben ser comunicados abiertamente.

## Sesgos 

- Infrarrepresentación de grupos por falta de datos
- Sesgo en los datos (p.ej. ligeras diferencias en las mediciones en distintos lugares,
etc.)
- Adecuación del modelo a los datos (puede ser buena elección para un grupo pero no para otro)


# Unidad 1. Introducción al aprendizaje automático

## Objetivos 

Los objetivos que se pretenden alcanzar en este recurso son los siguientes:
- Conocer las aplicaciones del aprendizaje automático.
- Comprender en qué consiste este proceso y cuál es su motivación.
- Entender el contexto del proceso y su relación, sobre todo, con otros conceptos como la inteligencia artificial, la ciencia de datos y otros relacionados

![](/img/aprendizaje_automatico/intro.png)

## Sistemas basados en conocimientos

![](/img/aprendizaje_automatico/sistema_basado_en_conocimiento.png)

![](/img/aprendizaje_automatico/sistema_basado_en_conocimiento_2.png)

## Aprendizaje automático 

> Aprendizaje automático: "Sistemas que aprenden a cambiar su comportamiento de modo que resulten m{as efectivos en el futuro". 

> A computer program is said to learn from experience E with respecto to some Task T and some performance measure P, if its performance on T, as measured by P, impreves with experience E." - Tom Mitchell, Carnegie Mellon University

### Ventajas e inconvenientes

![](/img/aprendizaje_automatico/ventajas_inconvenientes.png)

![](/img/aprendizaje_automatico/ejemplo_aprendizaje_auto.png)

### Léxico 

- Data mining - fase de aprendizaje propiamente dicho. 
- Data analysis fase de comprensión y preparación del dato previa al aprendizaje
- Data Science, ciencia de datos - Todos los procesos relacionados incluyendo con el aprendizaje y también de negocio, incluyendo desde la captura de requisitos y definiciión del caso de uso, análisis, aprendizaje autommático y reporting de resultados, hasta la operativización o industrialización de los modelos construidos. 



## El marco de la IA 

La inteligencia artificial es la ciencia que se encarga de desarrollar sistemas y de investigar cómo realizan los seres humanos unas determinadas tareas. Son sistemas que tienen que estar orientados a automatizar o a imitar las capacidades humanas en ámbitos en los que actualmente los sistemas o seres humanos son mucho más efectivos que las máquinas

![](/img/aprendizaje_automatico/IA.png)

## Conclusiones

- El objetivo final del AA es la automatización de tareas realizadas usualmente por seres humanos. 
- Se contrapone fundamentalmente a los sistemas expertos, en los que las decisiones se toman por medio de reglas introducidas por un ingeniero del concoimiento en base a un experto (médico, etc. )
- El AA es la capacidad de un sistema software de mejorar su rendimiento con la experiencia. 
- Son sistemas automáticos, portables, eficaces y escalables. 
- Se corresponde con la fase de minería de datos en el KDD, y está en el centro de la ciencia de datos y de la analítica. 
- Se corresponde con una de las capacidades humanas que son objeto de estudio por la IA. 

## Tipos de Aprendizaje Automático 

### Objetivos 

Los objetivos que se pretenden alcanzar en este recurso son los siguientes:

- Conocer los principales tipos de algoritmos de aprendizaje.
- Comprender la diferencia entre algoritmos supervisados y no supervisados.
- Ser capaces de identificar los problemas de clasificación y de regresión.
- Conocer los fundamentos de los algoritmos de agrupamiento de ejemplos y
variables.

### ¿Por qué utilizar Aprendizaje Automático?

sistemas de aprendizaje automático o machine learning son ideales para:
1. Problemas que requieren mucho trabajo manual o largas listas de reglas.
2. Entornos que evolucionan.
3. Conseguir insights en datos complejos.

En el siguiente ejemplo queremos clasificar el spam entrante. Nuestra aproximación más simple podría ser algo así: 


![](/img/aprendizaje_automatico/aprendizaje_automatico.png)

Pero...
▪ Los spammers se pueden adaptar…
▪ Las reglas cambiar…
▪ Obligarnos a iterar sobre los distintos pasos del sistema montado…

Si en vez de intentar definir las reglas a mano cada vez que queramos modificar nuestro sistema de detección de spam utilizamos un clasificador, podemos redefinir el problema como:
▪ Estudiar el problema desde el punto de vista de qué variables me afectan a que sea spam, por ejemplo, patrones de palabras frecuentes en mails normales o spam (bag of words, tf-idf).
▪ Entrenar un clasificador sobre las variables seleccionadas: e.g. naive-bayes, logistic regression...
▪ Evaluar el rendimiento y poner en producción

![](/img/aprendizaje_automatico/aprendizaje_automatico_2.png)


### Clasificación de los tipos de aprendizaje

#### En función de cómo aprenden

![](/img/aprendizaje_automatico/tipos%20de%20a_a.png)

#### En función de la actualización interna del algoritmo 

- **Online Learning.** Algoritmos que permiten actualizar su estado de forma incremental a medida que nueva información llega (ideal para Big Data o modelos que deben adaptarse rápidamentea a los cambios de manera autónoma). 
- **Batch Learning.** El sistema no es capaz de aprender de forma incremental y necesitamos todos los datos disponibles durante la fase de entreno. En el caso en el que queramos incorporar nueva información al modelo, no podremos añadirle simplemente los nuevos datos adquiridos, si no que tendremos que entrenar un modelo nuevo y reemplazar el antiguo. Al necesitar todos los datos en memoria puede ser caro a nivel computacional.

#### En función de la referencia que utilizan para clasificar/predecir

Hacemos una tercera clasificación de acuerdo a si nuestros algoritmos trabajan comparando nuevas instancias con las ya conocidas o bien detectar patrones para crear un modelo.

- Aprendizaje basado en modelos: Adapta parámetros del modelo a los datos y los generaliza a nuevas observaciones.
- Aprendizaje basado en instancias: Requiere una medida de similaridad. Evalúa cuan similar es un punto nuevo a uno de los existentes (k-nearest neighbours).


#### Aprendizaje Supervisado

Se habla de aprendizaje supervisado cuando se hace referencia a que el modelo construido está basado en unas observaciones sobre las que ya conocíamos la respuesta (es decir, todos los datos de entrenamiento están etiquetados) e intentamos averiguar qué responderíamos ante una serie de datos nuevos.

![](/img/aprendizaje_automatico/supervisado.png)

Grupos de aprendizaje supervisado:  clasificación y los algoritmos de regresión. La mejor forma de distinguir entre uno y otro es preguntarse si necesitamos predecir una cantidad o un valor continuo (cuánto vale una casa dado un vecindario y la superficie), o si queremos predecir una clase (dado el valor de una casa y la superficie dime en que vecindario está construida).

##### Algoritmos de Calsificación 

Se aplican si el tipo de la variable objetivo es categórica, nominal o escalar, es decir, toma una serie discreta de posibles valores, aunque estos puedan ser códigos numéricos. 

Cada uno de los posibles valores se considera una clase distinta y es por ello por lo que se llaman algoritmos de clasificación.

El problema puede ser binario o no, es decir, puede tener dos o más clases. 

Adicionalmente, las clases pueden ser solapadas o no, es decir, podría darse el caso de que un registro pertenezca a dos o más clases a la vez en determinados problemas (ej. noticias periodísticas). 

Entre los algoritmos más usados y útiles encontramos:
1. Logistic Regression
2. Regression trees and ensembles
3. Support Vector Classifiers

##### Algoritmos de Predicción Numérica o Regresión 

Los algoritmos de regresión o predicción numérica son aquellos algoritmos de aprendizaje supervisado que son capaces de predecir una variable objetivo-numérica real o continua.

Algunos algoritmos de regresión se llaman exactamente así, porque la regresión es un concepto estadístico de aproximación numérica de una serie de puntoss (regresión lineal, la regresión logística y la regresión Lasso). 

Otros algoritmos como los bayesianos (basados en probabilidades y el teorema de Bayes, como el algoritmo Bayes ingenuo o las redes bayesiana), devuelven probabilidades, que no dejan de ser aproximaciones numérica en el intervalo [0,1].

Los problemas de clasificación y los de regresión están muy relacionados entre sí. En ambos casos se trata de predecir el valor de una variable dependiente y con mucha frecuencia se pueden tratar problemas de clasificación como problemas de regresión, o a la inversa.

Por ejemplo, los algoritmos de predicción numérica que devuelven una probabilidad, como los algoritmos bayesianos, pueden usarse para la clasificación binaria. 

Entre los algoritmos más usados para realizar regresiones encontramos:
1. Linear Regression and variants (échale un vistazo a esto):
    a. Ridge Regression (l2)
    b. Lasso Regression (l1)
    c. Stepwise Regression
2. Regression trees and trees ensembles
3. Support Vector Regressors

#### Aprendizaje No Supervisado 

Los algoritmos de aprendizaje no supervisado están orientados a problemas en los que no existe una variable objetivo, es decir, todas las variables del problema se consideran independientes, y su finalidad es generar grupos de datos similares entre sí y distantes de los demás grupos.

Los algoritmos de aprendizaje no supervisado se utilizan para organizar los datos facilitando su análisis exploratorio y su comprensión, pero también pueden usarse para asimilar nuevos datos a los grupos existentes. Por ejemplo, en función de su similitud con el resto de los usuarios, podemos decidir que un nuevo usuario pertenece a un grupo o a otro.

Por tanto, este conocimiento o modelo de grupos generado también es accionable, es decir, se puede utilizar también de manera práctica, porque podemos aplicar a este nuevo dato el mismo tratamiento que estamos dando a los demás datos de su grupo, en fases posteriores de trabajo.

Los algoritmos de aprendizaje no supervisado se pueden organizar en dos grandes grupos: 
-  los que pretenden estructurar los ejemplos (agrupamiento de instancias) 
- los que pretenden estructurar las variables (agrupamiento de características). 

 Es decir, los algoritmos pueden abordar las filas o las columnas de nuestro dataset. 

 Podemos encontrar tres otras subcategorías en función del objetivo que definamos o del problema a cubrir:

 ##### Segmentación o CLustering

Los algoritmos de agrupamiento de instancias son algoritmos de clasificación en los que las clases no están predefinidas, es decir, se crean los grupos por similitud o distancia entre los elementos que componen dichos grupos, denominados clusters.

Un algoritmo de agrupamiento pretende generar grupos de ejemplos de forma que:
- Los elementos de cada grupo sean lo más parecidos posibles entre sí.
-  Los grupos sean lo más distantes posibles de los demás.

Un proceso relativamente frecuente en la aplicación de algoritmos de aprendizaje es la aplicación de algoritmos de clustering para encontrar grupos de ejemplos similares entre sí, y la aplicación subsiguiente de algoritmos de clasificación o regresión para la construcción de clasificadores especializados para cada uno de los grupos. De este modo frecuentemente se logra una mayor precisión o efectividad en cada uno de los grupos que aplicando el algoritmo de clasificación o regresión a la totalidad de los ejemplos de una sola vez.

Este tipo de algoritmos se enfrenta a dos dificultades principales:

1. Número óptimo de grupos: determinar el número óptimo de grupos, que se desconoce a priori (se selecciona el mejor grupo por medio de prueba y error y métricas de calidad)
2. Similitud o distancia: construir una métrica adecuada de similitud o distancia entre ejemplos que tenga en cuenta las características propias de las variables.


Algoritmos populares: 

1. k-means y variantes
2. (Hierarchical) Density-Based Spatial Clustering of Applications with Noise: (H)DBSCAN
3. Hierarchical clustering
4. Expectation Maximization (Gaussian mixture models...)


##### Algoritmos de agrupación de características

Estos algoritmos pueden ser tan simples como mediciones de correlación entre dos variables, de modo que se busque detectar que variables son redundantes, porque no aportan información sobre lo que ya dice otra variable. Además, es posible estudiar la correlación entre una variable independiente y la variable dependiente en un problema predictivo, de modo que se estime la capacidad predictiva de la variable independiente. 

Entre los algoritmos más usados y útiles para la reducción de dimensionalidad no encontramos:
1. Principal Component Analysis (PCA)
2. t-distributed Stochastic Neighbor Embedding (t-SNE)

#### Repositorios de problemas y aplicaciones 

1. Kaggle
2. OpenML
3. UCI Repository 
4. KDNuggets 

#### Clasificaciones y definiciones complementarias

- Los algoritmos simbólicos son los que adquieren y manipulan el conocimiento de manera explícita, a través de representaciones que son comprensibles para el ser humano, como los árboles de decisión o los sistemas de reglas.
- El método subsimbólico es el conocimiento adquirido a través del aprendizaje se encuentra implícito en una representación no directamente interpretable por los seres humanos, como por ejemplo en una red neuronal o una matriz de probabilidades de un método bayesiano.
- Algoritmos bioinspirados, que son aquellos que emulan los comportamientos encontrados en los seres vivos de la naturaleza. 
    - Algoritmos genéticos: son algoritmos de optimización que reflejan las leyes de la supervivencia de la naturaleza a través de un proceso de generación de individuos por cruce genético con alteraciones aleatorias, y de una función de ajuste que indica la capacidad de cada individuo de adaptarse a su entorno, de modo que en cada generación solo se reproducen los más aptos, es decir, los más próximos a resolver el problema objetivo.
    - Algoritmos de enjambre (swarm intelligence): concepto que hace referencia a los comportamientos colectivos de grupos de seres vivos como las aves o los peces. Estos algoritmos se basan en el principio de que el comportamiento individual de una entidad puede ser sumamente simple, pero al juntar a muchos individuos de la misma especie emerge un comportamiento colectivo mucho más sofisticado. Se pude considerar que los algoritmos de hormigas son un comportamiento de enjambre.


### Repositorios de problemas y aplicaciones

1. Kaggle
2. **OpenML:** este repositorio de problemas OpenML almacena casi 20.000 conjuntos de datos en los que se pueden aplicar distintas técnicas de aprendizaje, con más de 50 000 tareas ya definidas. 
3. **UCI Repository:** el repositorio de datasets de aprendizaje automático UCI Repository, almacena unos 400 conjuntos de datos donados para la ciencia. 
4. **KDNuggets:** es un índice de datasets albergados en sitios públicos y de listados de datasets. Es un sitio sumamente popular y en el que es posible encontrar enlaces a los sitios anteriores, y la clasificación de los problemas y algoritmos es completamente plana. A través de KDNuggets es posible encontrar otros repositorios de problemas más especializados, como problemas de grandes dimensiones (Big Data real) o problemas de un ámbito específico (por ejemplo, medicina).

### Clasificaciones y definiciones complementarias 

- **Algoritmos simbólicos** son los que adquieren y manipulan el conocimiento de manera explícita, a través de representaciones que son comprensibles para el ser humano, como los árboles de decisión o los sistemas de reglas.
- **Algoritmos subsimbólicos**:  el conocimiento adquirido a través del aprendizaje se encuentra implícito en una representación no directamente interpretable por los seres humanos, como por ejemplo en una red neuronal o una matriz de probabilidades de un método bayesiano.
- los algoritmos bioinspirados, son aquellos que emulan los comportamientos encontrados en los seres vivos de la naturaleza.
- **Algoritmos genéticos:** son algoritmos de optimización que reflejan las leyes de la supervivencia de la naturaleza a través de un proceso de generación de individuos por cruce genético con alteraciones aleatorias, y de una función de ajuste que indica la capacidad de cada individuo de adaptarse a su entorno, de modo que en cada generación solo se reproducen los más aptos, es decir, los más próximos a resolver el problema objetivo.
-**Algoritmos de enjambre (swarm intelligence)**: concepto que hace referencia a los comportamientos colectivos de grupos de seres vivos como las aves o los peces. Estos algoritmos se basan en el principio de que el comportamiento individual de una entidad puede ser sumamente simple, pero al juntar a muchos individuos de la misma especie emerge un comportamiento colectivo mucho más sofisticado. Se pude considerar que los algoritmos de hormigas son un comportamiento de enjambre.

## Principales proveedores de IA

### Objetivos 

Hacer un pequeño repaso por los principales proveedores de Inteligencia artificial, ver algún caso de uso y una introducción de manejo de datos masivos y su implicación.

### Principales proveedores

- AEye (<https://www.aeye.ai/>), es una empresa dedicada a crear algoritmos de inteligencia artificial utilizando sensores inteligentes definibles por software para el guiado de vehículos autónomos.
- AIBrain (<https://aibrain.com/>), es una empresa que construye soluciones de IA para smartphones y aplicaciones robóticas a lo largo de cuatro dimensiones:cognitiva, social, física y emocional. Su principal objetivo es construir una IA totalmente autónoma aplicando la resolución de problemas, el aprendizaje y la memoria.
- AlphaSense (<https://www.alpha-sense.com/>), empresa qu ha desarrollado un motor de búsqueda muy potente impulsado por IA y que es capaz de hacer búsqueda de información entre más de 2.000 fuentes de datos comerciales premium que incluyen información bursátil, transcripciones , presentaciones y revistas de noticias de comercio, toto estos está principalmente diseñado para empresas de inversión, bancos y compañías, dónde en la mayoría de la veces se tienen que tomar decisiones estratégicas en muy poco tiempo y donde la información para tomarlas es vital.
- Amazon, además del comercio marketplace, también ofrece servicios de IA orientados al consumidor y al negocio (Alexa). 


## Procesos

El AA consta de dos procesos básicos que son: 
- Entrenamiento 
- Predicción

![](/img/aprendizaje_automatico/procesos.png)

![](/img/aprendizaje_automatico/procesos_2.png)

**Es imprescindible evaluar sistemáticamente la calidad del aprendizaje para optimizarlo** introduciendo:
- Nuevos hiperparámetros. 
- Nuevas variables. 
- Otros ejemplos en el dataset de entrenamiento .
- Selección y extracción de variables. 

Si no somos capaces de evaluar la calidad de nuestro modelo, el modelo no sirve. 

![](/img/aprendizaje_automatico/procesos_3.png)


Cuando tenemos un modelo bien evaluado y con alta capacidad de representación, debemos pasar a la industrialización el proceso de creación y despliegue de los modelos. 


![](/img/aprendizaje_automatico/despliegue.png)

![](/img/aprendizaje_automatico/industrializacion.png)

Podemos reentrenar nuestros modelos sobre los errores o sobre todos los datoss nuevos, y automatizar el proceso de aprendizaje con AutoML. 

![](/img/aprendizaje_automatico/procesos_4.png)

Debemos reentrenar nuestros modelos periódicamente para hacer frente al Concept Drift. 

![](/img/aprendizaje_automatico/procesos_5.png)

![](/img/aprendizaje_automatico/procesos_6.png)

![](/img/aprendizaje_automatico/procesos_7.png)


# Unidad 2. Aprendizaje Supervisado 



## Métodos de predicción lineal 

### Objetivos 

- Conocer los principios generales de los métodos de predicción lineal.
- Conocer cómo funcionan en general los métodos más populares, especialmente los métodos de regresión lineal y support vector machines.
-  Saber cómo se utilizan los algoritmos anteriores en Scikit-learn.
- Conocer cómo se utilizan los algoritmos anteriores en Spark.

### 1. Introducción 

el método de predicción se llama lineal porque el modelo es una función lineal en la que ninguno de los términos en x está elevado a ninguna potencia. 

1. Para problemas de predicción numérica o regresión, en los que al sustituir el ejemplo objetivo se obtiene un valor que es la estimación para la variable objetivo en dicho ejemplo.
2. Para problemas de clasificación binaria, en los que después de aplicar la función objetivo es necesario usar además una regla de decisión de la forma f(𝑋̅)≥U para un umbral U determinado, de forma que clasificamos un ejemplo X como positivo (perteneciente a la clase) si el valor f(X)≥U, y como negativo en caso contrario.

Razones por la cual este algoritmo es popular: 
1. Métodos con un grado de eficacia alto.
2. Rápidos en la fase de entrenamiento. 
3. Modelo sencillo de interpretar.

Cada método está inspirado por principios distintos:
- Estadísticos, (en el caso de )la regresión lineal o logística)
- Probabilísticos, (Bayes Ingenuo).
- O algebraicos (SVM).


## Árboles de decisión 

### Introducción 

Los árboles de decisión son una buena representación del conocimiento: operativa, clara y sencilla.

Además, sus algoritmos de inducción son también relativamente sencillos: En cada paso, se elige un test que se configura como un nodo y se divide la colección de acuerdo con él, progresando de manera recursiva sobre cada subcolección generada por el test anterior.

### Random Forest

Un random forest no es más que un conjunto de árboles.

por qué no se obtienen resultados idénticos? Dos razones:
1. **Subsampling:** Cada árbol está entrenado con un subconjunto de datos seleccionado aleatoriamente
2. **Max Features:** El punto óptimo para la división viene de un conjunto de features seleccionadas aleatoriamente, controlado por el hiperparametro max_features.

### Gradient Boosting Trees

Los algoritmos de Gradient Boosting entrenan en los errores residuales. Lo hacen
entrenando de manera iterativa una secuencia de modelos, basados en el error residual
del modelo anterior. Cada modelo reduce el error residual que se encuentra en la
predicción anterior. 

### Feature importance

Afortunadamente, todos estos modelos una vez son
entrenados tienen un atributo llamado feature_importances_ que devolverá un array
con la importancia de las features.

## Deep Learning

### Objetivos

- Comprender los fundamentos del Deep Learning.
- Aplicar la técnica de backpropagation.

### Fundamentos 

#### ¿Qué es el Deep Learning? 

Deep Learning es una rama del Machine Learning, que intenta simular la estructura biológica y la funcionalidad de un cerebro humano usando redes de neuronas artificiales. Estas redes pueden incluir:
- Perceptrón multicapa.
- Redes neuronales convolucionales.
- Redes neuronales recurrentes.

Además, estas redes son jerárquicas o multicapa, siendo capaces así de modelar grandes abstracciones de datos. 

Uno de los principales beneficios de usar deep learning en contraposición con el clásico machine learning, es que el performance es que cuantos más datos, mejor modelo.


Algunas de las aplicaciones recientes del deep learning son:
- Generar captions.
- Resumir.
- Traducir texto.
- Generar audio.
- Producir arte.

#### El perceptrón 

Un perceptrón es un clasificador lineal que se entrena con aprendizaje iterativo.
El modelo funciona como sigue:
- Input: Un datapoint, este punto se transformará en un vector de longitud n, con cada elemento describiendo el valor de cada una de las features.
- Output: Una clasificación, -1 o 1. 

![](/img/aprendizaje_automatico/perceptron.png)


##### Funciones de activación 

- Sigmoide
- Tanh
- ReLu. Función más común y que tiene mejor desempeño 

## Problemas del Aprendizaje Predictivo 


### Objetivos

Los objetivos que se pretenden alcanzar en este recurso son los siguientes:
- Entender el problema del sobreajuste y cómo abordarlo.
- Comprender en qué consiste el desbalanceo de clases y cómo resolverlo.
- Aprender a abordar los problemas de costes de error asimétricos.
- Ser capaz de gestionar un problema de clases solapadas.
- Conocer las técnicas para la optimización de hiperparámetros.

### Introducción 

Los problemas más comunes en la práctica son: 


![](/img/aprendizaje_automatico/problemas.png)

### Sobreajuste

El problema del sobreajuste u overfitting consiste en que el modelo resulta ser muy efectivo sobre los datos de entrenamiento, pero mucho menos sobre los datos reales, de operación. Los motivos pueden ser: 

- Los datos de entrenamiento no representan adecuadamente a los datos reales.
- Los algoritmos se optimizan demasiado en el entrenamiento. 
- Falta de evaluación rigurosa y sistemática. 

Para evitar estos problemas se utilizan técnicas de mejora de los algoritmos que son específicas de cada uno de ellos. P. ej.: 

- Árboles de decisión: los árboles de decisión incluyen un método de poda o prunning que acorta las ramas demasiado largas que tienen pocos ejemplos en las hojas, por ser demasiado específicas. 
- Comités de clasificadores: los comités de clasificadores como el Boosting, Random Forest o el Stacking permiten construir múltiples modelos sobre subconjuntos de los datos de entrenamiento y combinarlos de manera eficaz para garantizar una variabilidad estadística del modelo global que los combina a todos.

### El desbalanceo de las clases

Este problema consiste en que los datos poseen una distribución de clases muy desequilibrada. 

Los algoritmos de aprendizaje normalmente disponen de algún mecanismo de optimización. 

El árbol de decisión se construye segmentando cada nodo o subcolección de datos por medio de un test que parte ese subconjunto de datos de manera óptima, y para ello utiliza métricas como la ganancia de información o la ratio de ganancia (gain ratio).

Para afrontar el problema del desbalanceo de clases, se pueden utilizar diversas técnicas:
- Procedimiento de estratificación para rebalancear las clases.
    - **Undersampling o inframuestreo:** eliminar ejemplos de la clase mayoritaria, hasta lograr una distribución más equilibrada. El principal problema de esta técnica es que perdemos datos de entrenamiento.
    - **Oversampling o sobremuestreo:** multiplicar aleatoriamente los datos de la clase minoritaria. Como no se pierden datos de entrenamiento, esta técnica se considera preferible al inframuestreo, aunque a cambio impacta en el coste en tiempo al aumentar el número de ejemplos.
- Aprendizaje sensible al coste de error. Sabiendo que casi todos los algoritmos están guiados por el error, es decir, intentan minimizarlo, se puede ajustar la métrica de error en la clase menos representada haciéndola más costosa para que el algoritmo se concentre más en clasificar correctamente nuestros ejemplos objetivo. 

### Clases solapadas

Este problema se produce en los casos de clasificación.

Para afrontar este problema, se puede binarizar el problema convirtiéndolo en N problemas distintos (donde N es el número de clases). Alternativamente, se pueden usar políticas de umbral o ranking. 

La binarización de problemas de clasificación se suele denominar one hot encoding. 

![](/img/aprendizaje_automatico/one_hot_encoding.png)

Para entrenar nuestro modelo, se construyen N datasets (4 en este caso), y se entrena y evalúa con cada uno por separado. El resultado de la evaluación es el promedio sobre los N problemas.

Otra alternativa es utilizar las políticas de umbral: Si nuestro tipo de modelo es capaz de arrojar un valor numérico, se puede establecer un umbral sobre el ranking de posibles valores y asignar a las clases aquellos valores que superan un determinado umbral.

Esta técnica tiene como desventaja que depende de que el algoritmo produzca probabilidades o valores numéricos de clasificación. Sin embargo, tiene una ventaja: puede ofrecer al usuario un “top” de clases probables, ordenadas, de modo que el usuario puede tomar una decisión más flexible.

Por ejemplo, un modelo para predicción de enfermedades puede ofrecer a un médico las tres enfermedades más probables de un paciente en función de los resultados de las pruebas que hemos realizado, incluso con un valor de probabilidad, y el médico puede diagnosticar con más base o incluso recomendar nuevas pruebas que puedan ayudar a realizar un diagnóstico 100 % efectivo.

### Optimización de hiperparámetros 

Todos los algoritmos relacionados con el aprendizaje automático tienen hiperparámetros.

Los hiperparámetros se llaman así porque son parámetros del algoritmo, ya que tradicionalmente se han considerado parámetros a las variables independientes del dataset.

# Unidad 4. Evaluación del aprendizaje

## Conceptos de evaluación 

### Objetivos

- Motivar la necesidad de evaluar el aprendizaje automático.
- Entender los distintos aspectos a evaluar en un proceso de este tipo.
- Comprender la relación entre eficacia y rendimiento.
- Entender el concepto de curva de aprendizaje.
- Conocer la importancia de la compacidad, expresividad e inteligibilidad de los modelos analíticos.

### 1. Introducción a los conceptos de evaluación

La evaluación de los algoritmos analíticos es un proceso absolutamente esencial. No se puede mejorar la calidad de lo que no se mide. 

Existen dos procesos básicos en la aplicación de un método de aprendizaje: 

1. **Proceso de entrenamiento:** durante este proceso, se aplica un algoritmo que construye un modelo sobre los datos de entrada o dataset de entrenamiento.
2. **Proceso de aplicación:** es el proceso de aplicación del modelo sobre un conjunto de datos nuevos, no vistos previamente.

#### ¿Cómo sabemos si el modelo que hemos construido es suficientemente bueno?

La capacidad de acierto de un modelo es importante, pero no es el único parámetro que se tiene que considerar y medir. Existe un balance entre los distintos aspectos que queremos evaluar.

1. **Eficacia:** la capacidad de acierto del mismo sobre los nuevos datos, o incluso sobre los propios datos de entrenamiento.
2. **Eficiencia**: Cómo de rápido es durante los procesos de entrenamiento y de aplicación.
3. **Se puede medir la compacidad y expresividad del modelo**: si un modelo es más corto, es más compacto (por ejemplo, tiene menos reglas de clasificación que otro), y por lo tanto es mejor. 
4. **Claridad o inteligibilidad del modelo:** si un modelo es más fácil de entender para un ser humano que otro modelo, el primero es mejor.

### 2. La evaluación de la eficacia

 Se trata de un aspecto medible de una manera relativamente fácil, si disponemos de un conjunto de datos de referencia que ya han sido clasificados o etiquetados por el ser humano.


Los algoritmos de aprendizaje pueden clasificarse en función de si son supervisados o no:

- Algoritmos de aprendizaje supervisado: los algoritmos o problemas de aprendizaje supervisado, ya sean de clasificación o de regresión, son los más fáciles de medir en términos de eficacia.
- Algoritmos de aprendizaje no supervisado: Se toma como referencia una clasificación manual realizada por un ser humano, pero también es posible definir métricas de calidad en términos de la cohesión o densidad de los grupos generados y su distancia a los demás grupos. Cuanto más parecidos entre si sean los ejemplos que hay dentro de cada grupo y más diferentes de los ejemplos de otros grupos, mejor será nuestro modelo no supervisado. 

Los algoritmos de aprendizaje típicamente son tanto más efectivos cuanto de más datos dispongan para su entrenamiento. 

![](/img/aprendizaje_automatico/curva_aprendizaje.png)

#### ¿Qué es mejor, más datos o mejores algoritmos? O planteado de otra forma, ¿es mejor conseguir más datos o diseñar mejores algoritmos para lograr niveles de efectividad más altos?

Después de décadas en las que la comunidad científica ha invertido considerable esfuerzo en el diseño de algoritmos cada vez más efectivos, el personal de Google invita a utilizar algoritmos relativamente simples sobre grandes cantidades de datos.

Su hipótesis es que hasta los algoritmos más simples pueden alcanzar la efectividad de los más sofisticados, suponiendo que los dotemos de la suficiente cantidad de datos. Por otro lado, el mismo Google es el promotor de la API de deep learning más popular en la actualidad, TensorFlow, que abordar problemas aún abiertos como el procesamiento del lenguaje natural o el reconocimiento de imagen,

No obstante no solo tener más datos importa, sino que estos sean de mayor calidad también es relevante. Y no solo la eficacia importa, porque trabajando con datasets de varios terabytes, el tiempo de entrenamiento o de clasificación puede ser tan alto que la complejidad del algoritmo impida su aplicación práctica. 

### La evaluación del rendimiento 

Cuando hablamos de rendimiento, hablamos fundamentalmente del tiempo de ejecución de un algoritmo, no de su eficacia o grado de acierto. 

Nos interesa conocer la eficiencia práctica de los algoritmos de aprendizaje, es decir, el tiempo que tardan en sus procesos de entrenamiento y predicción. Pero los algoritmos no deben ser eficientes solo en tiempo, sino también en el uso de otros recursos disponibles en el sistema, como la memoria o el disco. Un algoritmo puede ser muy eficiente en tiempo, pero requerir para ello una cantidad de memoria no disponible, por lo que el tiempo de acceso a disco lo va a ralentizar.

### La evaluación de la expresividad

- Un clasificador lineal se representa como una línea recta en un espacio bidimensional (dos variables independientes), o como un plano en un espacio tridimensional. En general, geométricamente se trata de un hiperplano. Estos clasificadores son capaces de hacer un buen trabajo en problemas linealmente separables. Un problema es linealmente separable cuando se puede construir un hiperplano que deje a un lado todos los ejemplos de una clase y al otro los de la contraria. Gráficamente,
- Árboles de decisión: se dice que los árboles de decisión tienen la expresividad suficiente como para abordar de manera eficaz problemas no linealmente separables.

### La evaluación de la compacidad 


Un modelo compacto es aquel que construye un número mínimo de hipótesis sobre los datos.

En general, es más fácil comparar la compacidad de las distintas expresiones de un algoritmo.

### La claridad e inteligibilidad 

Los algoritmos de aprendizaje deben dar lugar a modelos capaces de realizar predicciones o tomar decisiones sobre los datos, pero dichas decisiones deben ser comprensibles para los seres humanos ya que no van a tomar decisiones fundamentándose sobre un algoritmo que no entienden su proceso. 

La **claridad** de un modelo es importante porque permite a un ser humano tomar decisiones ejecutivas basadas en el modelo con confianza.

La **compacidad** de un modelo se mide en términos de la teoría de la información y tiene aplicación para comparar modelos desde el punto de vista de la navaja de Occam. Los modelos más simples y compactos son preferibles porque son más fáciles de entender y menor propensos al sobreajuste.

## 2. Evaluación del aprendizaje supervisado

### Objetivos
Los objetivos que se pretenden alcanzar en este recurso son los siguientes:
- Entender las fases del aprendizaje y fases a evaluar.
- Conocer las métricas más relevantes para la regresión.
- Conocer las métricas más relevantes para la clasificación.
- Conocer las técnicas de selección de modelos.
- Conocer las técnicas de optimización de hiperparámetros

### 1. Evaluación del aprendizaje supervisado

Como hemos visto anteriormente, la efectividad, que mide el grado de acierto de un sistema, es el aspecto de la evaluación al que más atención se presta habitualmente en la evaluación del aprendizaje automático.

#### Protocolo de evaluación 

El protocolo de evaluación define cómo se van a obtener las muestras para evaluar un sistema. 

![](/img/aprendizaje_automatico/protocolo-evaluacion.png)

Determinan cómo se obtiene el dataset de evaluación, cuando típicamente partimos de un dataset global etiquetado (clasificación) o estimado (regresión) manualmente.

Con carácter general, existen tres protocolos de evaluación:

1. Evaluación sobre el dataset de entrenamiento (training set): Se puede usar como primera aproximación porque lógicamente un modelo tiene que ser eficaz sobre aquello mismo sobre lo que ha aprendido.Sirve para descartar aquellos algoritmos que den muy malos resultados.
2. Evaluación sobre un subconjunto de prueba (test set): 
    - Dividimos el dataset de entrenamiento en dos subconjuntos, entrenamiento y evaluación. 
    - Aseguramos que la proporción de clases o valores es similar en ambos subconjuntos.
    - Entrenamos sobre el subconjunto de entrenamiento.
    - Calculamos las predicciones sobre el subconjunto de evaluación o prueba.
    - Contrastamos lo de que dice el clasificador y lo que originalmente vale la variable objetivo con una métrica de evaluación. 

#### Sesgo y varianza 

![](/img/aprendizaje_automatico/eeror.png)

- Sesgo: La diferencia entre el valor predicho por el modelo y el valor real.

![](/img/aprendizaje_automatico/sesgo.png)

- Varianza: El error producido debido a la sensibilidad del modelo con respecto a los datos de entreno.

![](/img/aprendizaje_automatico/varianza.png)

#### Compromiso entre sesgo y varianza 
Los modelos paramétricos, tipo los lineales, son modelos con un alto bies y una baja varianza. Por otro lado, los algoritmos no paramétricos, tipo los árboles de decisión, son modelos con bajo bies y alta varianza.

![](/img/aprendizaje_automatico/sesgo-varianza.png)

![](/img/aprendizaje_automatico/sesgo-varianza-2.png)

#### Validación cruzada 

Se considera el método estándar para el cálculo de métricas de evaluación en algoritmos supervisados o predictivos.

Surge para aliviar los problemas del protocolo del subconjunto de evaluación. Este protocolo
esencialmente consiste en repetir el protocolo de evaluación sobre un subconjunto de
prueba varias veces y promediar la medición de eficacia. 

- Dividimos el dataset de entrenamiento en K grupos aleatorios (con valores de
K típicos 3,5 o 10).
- Efectuamos K pruebas, en cada una reservamos un grupo distinto para evaluación y K-1 grupos para entrenamiento.
- Entrenamos sobre K-1 grupos, clasificamos el grupo restante y medimos sobre
el grupo restante.
- Promediamos los K resultados obtenidos.

##### Ventajas sobre train-test

- No evalúa nunca sobre los datos de entrenamiento.
- No depende de la aleatoriedad.
- Se aprovechan todos los datos para entrenar y evaluar.

#### Ajuste de hiperparámetros 

Podemos ajustar los hiperparámetros de ciertos modelos para aumentar o disminuir su "flexibilidad" a la hora de adaptarse a los datos y seleccionar el mejor a partir del resultado de la validación cruzada.

Automatizamos este proceso por fuerza bruta definiendo un espacio de búsqueda sobre los distintos hiperparámetros que queremos probar y calculando el error cometido para cada una de las combinaciones, con herramientas como GridSearchCV y RandomSearchCV. 

![](/img/aprendizaje_automatico/mejora_hiper.png)

### 2. Métricas de efectividad para predicción numérica

Las métricas más habituales en el caso de la predicción numérica o regresión son: 

- El error total. 
- El error medio. 

- El error absoluto

![](/img/aprendizaje_automatico/absoluto.png)

- Error absoluto medio: Es más robusto a los valores extremos y su interoperabilidad es más alta que la del RMSE ya que también está en las unidades de la variable a predecir con la ventaja de que el dato no ha sufrido ninguna transformación.

![](/img/aprendizaje_automatico/mae.png)

- El error cuadrático

![](/img/aprendizaje_automatico/error_cuadratico_medio.png)

- Error cuadrático medio 

![](/img/aprendizaje_automatico/error_cuadratico_medio_2.png)

- Raíz del error cuadrático medio : tiene el inconveniente de ser sensible a los valores extremos (outliers).

![](/img/aprendizaje_automatico/rmse.png)

- La varianza del error también se considera importante.

Evidentemente, en todos los casos, cuanto más próximo a cero sea el error, más preciso y efectivo se considera el modelo. 

#### Coeficientes de determinación (R2)

El coeficiente determina la calidad del modelo para replicar los resultados, y la proporción de variación de los resultados que puede explicarse por el modelo.

![](/img/aprendizaje_automatico/r2.png)

Consideraciones sobre R²:
1. R² no puede determinar si los coeficientes y las predicciones tienen sesgo. 
2. Cada vez que añadimos un predictor a un modelo, el R² aumenta, aunque sea de forma aleatoria, pero nunca decrece.

Para evitar el efecto del aumento de R² al agregar variables, utilizamos R2 ajustado, una versión que penaliza el resultado con el número de parámetros.

![](/img/aprendizaje_automatico/r2_ajustado.png)

![](/img/aprendizaje_automatico/r2_r2ajustado.png)

### Métricas de efectividad para clasificación

La elección de la métrica vendrá determinada por:
1. El tipo de predicción: si es una clase o la probabilidad de pertenecer a una clase.
2. Si se trata de un problema balanceado o no.

#### Matriz de confusión 

![](/img/aprendizaje_automatico/matriz_confusion.png)

Los aciertos se corresponden con las celdas en las que encontramos valores ciertos (ya sean positivos, que están en K, o negativos, que no deben estar en K). Los aciertos se corresponden con la diagonal principal.

Los errores se corresponden con las celdas en las que encontramos valores falsos. Los errores se localizan en la diagonal contraria.

#### Métricas

- Accuracy: Aciertos dividido a la muestra total. La más susceptible a darnos un clasificador erróneo, sobre todo en sets de datos no balanceados.

![](/img/aprendizaje_automatico/accuracy.png)

- Error rate

![](/img/aprendizaje_automatico/error_rate.png)

- Recall (sensitivity): es el número de ejemplos de la clase clasificados correctamente sobre el número total de ejemplos de la clase.

![](/img/aprendizaje_automatico/recall.png)

- Precision: Se corresponde con el número de ejemplos de la clase clasificados correctamente sobre el número total de ejemplos clasificados en esa clase. 

![](/img/aprendizaje_automatico/precision.png)

- Specifity

![](/img/aprendizaje_automatico/specifity.png)

#### F1-score

Combina la precisión y el recall en una sola métrica para comparar de forma sencilla dos clasificadores por medio de su media harmónica. De esta forma se da más peso a los valores bajos por lo que sólo se conseguirá un F1-score alto si ambas, Precision y Recall, son altas.

![](/img/aprendizaje_automatico/f1.png)

#### Curvas precision-recall

![](/img/aprendizaje_automatico/precision-recal.png)

#### Curvas ROC

La curva ROC (Receiver Operating Characteristic) es otra herramienta típica usada en clasificadores binarios. 

la curva ROC compara el True Positive Rate (Recall) y el False Positive Rate (specifity).


Otra forma de comparar clasificadores es comparando el área bajo la curva ROC (Area Under the Curve, AUC). Este parámetro tiene cómo límite superior 1, mientras que un valor de 0.5 corresponde a un clasificador completamente aleatorio.

![](/img/aprendizaje_automatico/auc.png)

En casos no balanceados o cuando una clase nos importa más que la otra, la curva PR nos aportará más información que la curva ROC.

## 3. Shap Values

### Objetivos

Describir como la técnica de Shap Values actúa como herramienta para conseguir la interpretabilidad de un modelo.

### Shap Values

En muchas ocasiones no podremos entregar un modelo que sea una caja negra, donde no podamos explicar que está pasando realmente.

cuanto mejor sea la interpretabilidad del modelo, mejor adopción tendremos.

Para conseguir esta interpretabilidad de los modelos, podemos utilizar distintas herramientas:
- Shap Values.
- Lime.
- InterpretML.
- ELI5.

SHAP significa Shapely Additive exPlanations, de este modo, para entender que son los SHAP values tenemos que saber que es un Shapely value.

Lundberg & Lee (2016) propusieron el SHAP value como una aproximación unida para explicar los resultados de cualquier modelo de machine learning, otorgándonos los siguientes beneficios:
1. Interpretabilidad global. Los valores agregados de SHAP values nos indica cuanto contribuye cada predictor.
2. Interpretabilidad local. Cada observación tiene su conjunto de SHAP values, lo que nos da transparencia.
3. Posibilidad de calcular SHAP para cualquier modelo basado en árboles.

Una cosa que podemos hacer con SHAP es obtener el signo del impacto, es decir, saber si las variables tienen un impacto positivo o negativo en el resultado final, vamos a verlo

![](/img/aprendizaje_automatico/shap_values.png)

## Evaluación del aprendizaje no supervisado

### Objetivos 

- Conocer cuáles son los aspectos para evaluar en aprendizaje no supervisado de manera comparada con los del supervisado.
- Entender en que consiste la evaluación directa versus la indirecta.
- Comprender las diferencias entre evaluación intrínseca y extrínseca.
- Ser capaz de entender el significado y mecánica del cálculo de las métricas de efectividad.
- Saber cómo calcular las medidas más relevantes en Scikit-learn y en Spark.

### Introducción 

En el aprendizaje no supervisado no existe una variable objetivo, es decir, todas las variables del problema se consideran independientes y su finalidad es generar grupos de datos similares entre sí y distantes de los demás grupos.

Los algoritmos de aprendizaje no supervisado se pueden organizar en dos grandes grupos, que son los algoritmos que pretenden agrupar los ejemplos (las filas) y los que pretenden estructurar las variables (las columnas). 

El agrupamiento de ejemplares se suele utilizar: 
- en el análisis exploratorio de datos, con el objetivo de entender cuáles son los patrones de comportamiento de nuestra colección de datos. 
- como apoyo a otra tarea, por ejemplo, de aprendizaje supervisado.

El agrupamiento de variables se puede utilizar: 
- como técnica de reducción de la dimensionalidad del dataset.
- para obtener una representación que facilite el agrupamiento de ejemplos,

En términos generales se puede decir que la efectividad es menos crítica en el agrupamiento, mientras que la claridad de los grupos generados es un aspecto central. 


### Evaluación de la efectividad 

Evaluacíon directa: En la agrupación de filas la tarea es típicamente un fin en sí misma, aunque se pueda utilizar como tarea de apoyo a otras tareas.
Evaluación indirecta: El agrupamiento de variables están asociado a procesos posteriores de aprendizaje predictivo, en el que interesa medir la efectividad. Por tanto, un algoritmo de agrupamiento de variables es tanto mejor cuanto más efectivo es el aprendizaje posterior.

En lo que se refiere a la evaluación de la efectividad del agrupamiento, típicamente se evalúa de manera directa y cuantitativa la efectividad del agrupamiento de ejemplos, mientras que el de variables se evalúa de manera indirecta.

### Métricas de Evaluación Intrínsecas

La evaluación intrínseca mide la calidad del agrupamiento sin tener como referencia un agrupamiento manual óptimo. La medida intrínseca más habitual es el coeficiente silhouette, que mide la densidad de los grupos y su separación con los grupos cercanos.

### Métricas de Evaluación Extrínsecas

La evaluación extrínseca precisa del conocimiento de una asignación o juicio de pertenencia realizado manualmente por un experto. En la evaluación extrínseca, se utilizan medidas habituales del aprendizaje supervisado como la medida F, pero otras propias del agrupamiento son más comunes.

### Cálculo de métricas

La medida posiblemente más popular es la información mutua ajustada, proveniente de la teoría de la información. Esta medida indica el grado de consenso entre dos asignaciones, la manual y la que se está evaluando, y es simétrica.


# Unidad 6. Ética de la Inteligencia Artificial

## 1. Directrices éticas para el desarrollo de modelos de IA

### Objetivos 

- Comprender la necesidad de incorporar un componente ético al desarrollo y uso de modelos de procesamiento automático.
- Conocer los principios éticos que deben regir a la inteligencia artificial.
- Conocer los requerimientos a desarrollar en el diseño, implementación y uso de servicios basados en la inteligencia artificial.

### 1. Introducción a la ética en el aprendizaje automático

Además de que sean potentes y escalables, los algoritmos de IA deben ser: 


- Transparentes a la inspección. 
- Predecibles para aquellos a los que gobiernan. 
- Robustos contra la manipulación. 
- La persona responsable. 

### 2. Principios éticos en el contexto de sistemas de IA 


Los sistemas de IA deben mejorar el bienestar individual y colectivo.

#### 2.1. Principio de respeto de autononía humana

Los derechos fundamentales en los que se basa la UE están orientados a garantizar el respeto de la libertad y la autonomía de los seres humanos. Los seres humanos que interactúan con los sistemas de IA deben poder mantener una autodeterminación plena y efectiva sobre sí mismos, y ser capaces de participar en el proceso democrático.

La asignación de funciones entre los seres humanos y los sistemas de IA debe seguir los principios de diseño centrados en el ser humano y dejar una oportunidad significativa para la elección humana. Esto significa asegurar el control humano sobre los procesos de trabajo en los sistemas de IA.


#### 2.2. El principio de prevención de daños

Los sistemas de IA no deben causar ni agravar los daños ni afectar negativamente a los seres humanos. Esto implica la protección de la dignidad humana, así como de la integridad mental y física.

Los sistemas de IA y los entornos en los que operan deben ser seguros y protegidos. Deben ser técnicamente robustos y debe garantizarse que no se presten a un uso malintencionado.

#### 2.3. El principio de equidad

El desarrollo, el despliegue y el uso de los sistemas de IA deben ser justos. La equidad tiene una dimensión sustantiva y otra de procedimiento.

#### 2.4. El principio de explicabilidad

La explicabilidad es crucial para crear y mantener la confianza de los usuarios en los sistemas de IA. Esto significa que los procesos deben ser transparentes, las capacidades y el propósito de los sistemas de IA deben ser comunicados abiertamente y las decisiones, en la medida de lo posible, deben ser explicadas a los afectados directa e indirectamente.

El grado de exigencia de la explicabilidad depende en gran medida del contexto y de la gravedad de las consecuencias si el resultado es erróneo o inexacto.


#### 2.5. Equilibrio entre los principios 

Pueden surgir tensiones entre los principios anteriores, para las que no existe una solución fija.

Aunque los principios anteriores ofrecen ciertamente una orientación para las soluciones, siguen siendo prescripciones éticas abstractas. Sin embargo, puede haber situaciones en las que no se puedan identificar compensaciones éticamente aceptables. Algunos derechos fundamentales y principios correlativos son absolutos y no pueden someterse a un ejercicio de equilibrio (por ejemplo, la dignidad humana).

#### 2.6. Requisitos para una IA confiable

Los distintos grupos de partes interesadas tienen diferentes funciones que desempeñar para garantizar el cumplimiento de los requisitos:
1. Los desarrolladores deben implementar y aplicar los requisitos a los procesos de diseño y desarrollo;
2. Los implantadores deben garantizar que los sistemas que utilizan y los productos y servicios que ofrecen cumplen los requisitos;
3. Los usuarios finales y la sociedad en general deben estar informados de estos requisitos y poder exigir su cumplimiento.

## 2. Detección de sesgos

### Objetivos

Los objetivos que se pretenden alcanzar en este recurso son los siguientes:
- Comprender el concepto de sesgo y equidad, a través de un acercamiento tanto cualitativo como cuantitativo.-  Reconocer algoritmos de detección y eliminación de sesgos.

### 1. Introducción a la detección de sesgos (bias)

El creciente uso de la inteligencia artificial en áreas sensibles, como la contratación, la justicia penal o la atención sanitaria, ha suscitado un debate sobre la parcialidad y la equidad. La toma de decisiones humana en estos y otros ámbitos también puede ser defectuosa y estar condicionada por prejuicios individuales y sociales que a menudo son inconscientes. ¿Serán las decisiones de la IA menos parciales que las humanas? ¿O empeorará la IA estos problemas?

a pesar de los problemas de definición de qué es en realidad el sesgo, muchos expertos tienden a acoger los algoritmos como un antídoto contra los sesgos humanos que siempre han existido. Al mismo tiempo, a muchos les preocupa que los algoritmos puedan incorporar y ampliar los sesgos humanos y sociales

### Sesgo y equidad en la IA

La discriminación es el trato desigual que reciben los individuos de determinados grupos, lo que hace que los miembros de un grupo se vean privados de beneficios u oportunidades. Los grupos más comunes que sufren discriminación son los basados en la edad, el género, el color de la piel, la religión, la raza, la lengua, la cultura, el estado civil o la condición económica.

Dado que los algoritmos de aprendizaje automático se utilizan cada vez más para determinar resultados importantes en el mundo real, como la aprobación de préstamos, las tasas de pago y las decisiones de libertad condicional, corresponde a la comunidad de IA minimizar la discriminación involuntaria. Esta discriminación involuntaria que se produce cuando una decisión tiene resultados muy diferentes para distintos grupos se conoce como impacto dispar (disparate impact).

1. **La adecuación de los datos para representar los diferentes grupos.** Los modelos de generalización a menudo ponderan a la baja los patrones infrecuentes y específicos, lo que puede resultar en un tratamiento injusto de los registros de las minorías. Esta falta de datos no se debe únicamente al tamaño reducido de los grupos minoritarios, sino también a la metodología de recopilación de datos que puede excluir o perjudicar a ciertos grupos, como cuando se recopilan datos solo en un idioma específico.
2. **El sesgo inherente a los datos.** Aunque la cantidad de datos sea suficiente para representar a cada grupo, los datos de formación pueden reflejar prejuicios existentes (por ejemplo, que las trabajadoras cobren menos), y esto es difícil de eliminar. Esta injusticia histórica en los datos se conoce como legado negativo.
3. **La adecuación del modelo para describir cada grupo.** La arquitectura del modelo puede describir algunos grupos mejor que otros. Por ejemplo, un modelo lineal puede ser adecuado para un grupo, pero no para otro.

### Definiciones sobre equidad

Se utiliza para referirse a un sesgo injusto, no deseado o indeseable, es decir, a la discriminación sistemática contra determinadas personas o grupos de personas basada en el uso inadecuado de ciertos rasgos o características.

La ausencia de prejuicios no deseados no es suficiente para concluir que un sistema es justo. 

En este caso, un modelo se considera justo si los errores se distribuyen de forma similar entre los grupos protegidos, aunque existen otras muchas formas de definirlo.

La mayoría de las definiciones de equidad se basan en la equidad de grupo, que trata de la equidad estadística en toda la población. Como complemento, está la equidad individual, que establece que los individuos similares deben ser tratados de forma similar, independientemente de su pertenencia al grupo.

- **Paridad demográfica**, o estadística, sugiere que un predictor es insesgado si la predicción 𝑦̂ es independiente del atributo protegido p de modo que: Pr(𝑦̂|𝑝)=Pr (𝑦̂)
- **igualdadd de probabilidades:** se satisface si la predicción 𝑦̂ es condicionalmente independiente del atributo protegido p, dado el valor verdadero y. Pr(𝑦̂|𝑦,𝑝)=Pr (𝑦̂|𝑦) 
- **igualdadd de oportunidades.** La igualdadd de oportunidades tiene la misma formulación matemática que la igualdadd de probabilidades, pero se centra en una etiqueta particular y=1:  Pr(𝑦̂|𝑦=1,𝑝)=Pr (𝑦̂|𝑦=1). 
    - La desviación de la igualdadd de oportunidades se mide por la diferencia de oportunidades:SPD= Pr(𝑦̂=1|𝑦=1,𝑝=1)−Pr (𝑦̂=1|𝑦=1,𝑝=0). 

### Algoritmos de mitigación de sesgo

Es muy difícil eliminar el sesgo una vez que el clasificador ya ha sido entrenado, incluso para casos muy simples.

Existen enfoques para tratar el sesgo en todas las etapas de la recogida de datos, el preprocesamiento y el proceso de entrenamiento. En esta sección se examinan algunos de estos métodos.

![](/img/aprendizaje_automatico/mitigacion_sesgo.png)

####  4.1. Enfoques en el preprocesamiento

Por desgracia, la supresión del atributo protegido y otros elementos que se sospecha que tienen información relacionada rara vez es suficiente. A menudo existen correlaciones sutiles en los datos que permiten reconstruir el atributo protegido.

Por ejemplo, podríamos suprimir la raza, pero si se conserva la información sobre la dirección del sujeto, esta podría estar fuertemente correlacionada con la raza.



**El grado de dependencia entre los datos x y el atributo protegido p se conoce como prejuicio latente**, y puede medirse mediante la información mutua. A medida que esta medida aumenta, el atributo protegido se vuelve más predecible a partir de los datos.

![](/img/aprendizaje_automatico/prejuicio_latente.png)


A continuación, analizaremos cuatro enfoques para eliminar el sesgo mediante la manipulación del conjunto de datos: 

1. **Manipulación de etiquetas**: Modifican las etiquetas y. Se calcula un clasificador en el conjunto de datos original y se encuentran ejemplos cercanos a la superficie de decisión. A continuación, se cambian las etiquetas de forma que sea más probable un resultado positivo para el grupo desfavorecido y se vuelve a entrenar. Se trata de un enfoque heurístico que mejora empíricamente la equidad a costa de la precisión. 
2. **Manipulación de los datos observados**:  Modifica los datos observados x. Se propone manipular las dimensiones individuales de los datos x de una manera que depende del atributo protegido p. Se alinean las distribuciones acumulativas F0[x] y F1[x] para la característica x cuando el atributo protegido p es 0 y 1 respectivamente a una distribución acumulativa mediana Fm[x]. También se denomina eliminación del impacto dispar.
3. **Manipulación de etiquetas y datos.** Se aplica una transformación aleatoria Pr(x′,y′|x,y,p) que transforma los pares de datos {x,y} en nuevos valores de datos {x′,y′} de una manera que depende explícitamente del atributo protegido p. A diferencia de la eliminación de impactos dispares, esto tiene en cuenta las interacciones entre todas las dimensiones de los datos. La transformación aleatoria, que también debe aplicarse a los datos de prueba, también viola la equidad individual.
4. **Reponderación de los pares de datos.** Se propone reponderar las tuplas {x,y} en el conjunto de datos de entrenamiento para que los casos en los que el atributo protegido p predice que el grupo desfavorecido obtendrá un resultado positivo, tengan una mayor ponderación.

#### 4.2. Enfoques durante el entrenamiento

Se puede utilizar el prejuicio indirecto, donde podemos medir la dependencia entre las etiquetas y, así como el atributo protegido p:

![](/img/aprendizaje_automatico/enfoque_entrenamiento.png)

Si no hay forma de predecir las etiquetas a partir del atributo protegido y viceversa, entonces no hay posibilidad de sesgo.

**Adversarial de-biasing:** reduce la evidencia de los atributos protegidos en las predicciones, tratando de engañar simultáneamente a un segundo clasificador que intenta adivinar el atributo protegido p.

![](/img/aprendizaje_automatico/ADVERSARIAL_DEBIASING.png)

**Eliminación de prejuicios mediante la regularización:** aplicados a casos de regresión logística, se propone añadir una condición de regularización extra a la salida del clasificador que intenta minimizar la información mutua entre el atributo protegido y la predicción 𝑦̂.

## 3. Ejemplos de dilemas éticos

### Objetivos

- Analizar cómo van a evolucionar algunos sectores productivos gracias a la aplicación de técnicas de inteligencia artificial.
- Comprobar cómo la evolución de los sectores puede provocar dilemas éticos que no se habían tenido en consideración hasta ahora.

### Problemas de sesgo en el reconocimiento facial

En 2018, investigadores del MIT y Microsoft revelaron que los algoritmos de clasificación de género presentaban tasas de error significativamente diferentes según la etnia y el género. Los algoritmos tenían una precisión del 1% para los hombres blancos, pero un 35% de error para las mujeres de piel oscura. Una investigación más exhaustiva en 2019 confirmó que la mayoría de los algoritmos mostraban disparidades demográficas en las tasas de falsos positivos y negativos. Se descubrió que los factores demográficos tenían un mayor impacto en las tasas de falsos positivos, lo cual es preocupante ya que aumenta el riesgo de identificar erróneamente a alguien. Además, se encontró que los asiáticos, afroamericanos e indios americanos tenían tasas de error más altas que los individuos blancos, y las mujeres tenían tasas más altas que los hombres. Para reducir el sesgo, la selección de datos de entrenamiento utilizados para construir modelos algorítmicos resulta crucial. La UE ha propuesto regulaciones que exigen conjuntos de datos de entrenamiento amplios y representativos. Sin embargo, el sesgo también puede manifestarse en las listas de vigilancia utilizadas en comparación con los sistemas de reconocimiento facial. Es importante cambiar la conversación sobre los riesgos del reconocimiento facial, ya que los mayores riesgos provienen de un funcionamiento preciso de la tecnología. A medida que se mejora la tecnología y los datos de entrenamiento, se eliminarán gradualmente los sesgos existentes, pero también surgirán nuevas preocupaciones. La gobernanza y gestión de la tecnología serán fundamentales para abordar estos desafíos en el reconocimiento facial.

### Vehículo autónomo 

Los vehículos autónomos tienen el potencial de revolucionar el transporte al reducir los accidentes de tráfico y ofrecer beneficios como ahorro de vidas, reducción de daños y mayor productividad. Sin embargo, surgen preocupaciones en áreas como la recopilación y el uso de datos, el impacto en el empleo y los desafíos éticos y de responsabilidad.

En términos de datos, los sensores utilizados por los vehículos autónomos pueden recopilar información personal sin consentimiento y no hay regulaciones claras sobre su recopilación, almacenamiento y acceso. Esto plantea preocupaciones sobre el uso indebido de los datos y las implicaciones en la toma de decisiones de los sistemas autónomos.

En cuanto al empleo, si los vehículos autónomos reemplazan a los conductores, se podrían perder muchos puestos de trabajo en ese campo. También se podrían ver afectados otros sectores, como seguros y reparaciones de colisiones, lo que podría generar desafíos económicos y de transición laboral.

En términos éticos y de responsabilidad, los vehículos autónomos plantean dilemas similares al "problema del tranvía", donde se debe tomar una decisión moral en situaciones de riesgo. Además, surgen preguntas sobre quién es legalmente responsable en caso de accidentes y cómo se deben establecer las normas éticas y legales para la interacción entre humanos y máquinas inteligentes.

Para abordar estos desafíos, se requiere la implementación de regulaciones actualizadas que prioricen la ética en el diseño y la adopción de los vehículos autónomos. También se necesita transparencia en los algoritmos y datos utilizados por estos sistemas para fomentar la confianza y la rendición de cuentas.

### eHealth 

Gracias a su capacidad para extraer conocimientos y aprender de grandes conjuntos de datos clínicos, la IA puede desempeñar un papel en el diagnóstico, la toma de decisiones y la medicina personalizada. Al mismo tiempo, crea un conjunto novedoso de desafíos éticos que deben ser identificados y mitigados.

### Automatización de trabajo 

La implantación de la inteligencia artificial (IA) en los procesos de producción plantea una amenaza para numerosos puestos de trabajo, ya que los robots inteligentes y los programas informáticos son más eficientes, cometen menos errores y son más baratos que los humanos. La IA puede adaptarse a nuevas condiciones y no requiere intervención humana en muchos casos. Sin embargo, los economistas argumentan que el progreso tecnológico no conduce necesariamente al desempleo masivo. A lo largo de la historia, la innovación de procesos y la innovación de productos han ido de la mano, creando nuevos puestos de trabajo en sectores emergentes. Este proceso de desarrollo es la causa del crecimiento económico a largo plazo. No obstante, el progreso tecnológico también implica el desempleo estructural, donde las personas desplazadas por las máquinas deben buscar empleo en otros sectores y adquirir nuevas habilidades. Los escenarios pesimistas predicen que la creación de empleo no podrá seguir el ritmo de los avances tecnológicos de la IA, lo que podría dar lugar a un desempleo tecnológico masivo. La automatización acelerada podría superar la capacidad de adaptación de los mercados laborales, generando un impacto distinto en comparación con revoluciones económicas anteriores.

